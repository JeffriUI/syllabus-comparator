[
    {
        "url": "https://www.coursera.org/specializations/big-data",
        "original_title": "Big Data Specialization",
        "translated_title": "Big data specialization",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "********* Do you need to understand big data and how it will impact your business? This Specialization is for you. You will gain an understanding of what insights big data can provide through hands-on experience with the tools and systems used by big data scientists and engineers. Previous programming experience is not required! You will be guided through the basics of using Hadoop with MapReduce, Spark, Pig and Hive. By following along with provided code, you will experience how one can perform predictive modeling and leverage graph analytics to model problems. This specialization will prepare you to ask the right questions about data, communicate effectively with data scientists, and do basic exploration of large, complex datasets. In the final Capstone Project, developed in partnership with data software company Splunk, you’ll apply the skills you learned to do basic analyses of big data. Interested in increasing your knowledge of the Big Data landscape?  This course is for those new to data science and interested in understanding why the Big Data Era has come to be.  It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems.  It is for those who want to start thinking about how Big Data might be useful in their business or career.  It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world! At the end of this course, you will be able to:\n\n* Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. \n\n* Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.\n\n* Get value out of Big Data by using a 5-step process to structure your analysis. \n\n* Identify what are and what are not big data problems and be able to recast big data problems as data science questions.\n\n* Provide an explanation of the architectural components and programming models used for scalable big data analysis.\n\n* Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model.\n\n* Install and run a program using Hadoop!\n\nThis course is for those new to data science.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  \n\nHardware Requirements:\n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size.  \n\nSoftware Requirements:\nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. Once you’ve identified a big data issue to analyze, how do you collect, store and organize your data using Big Data solutions?  In this course, you will experience various data genres and management tools appropriate for each.  You will be able to describe the reasons behind the evolving plethora of new big data platforms from the perspective of big data management systems and analytical tools.  Through guided hands-on tutorials, you will become familiar with techniques using real-time and semi-structured data examples.  Systems and tools discussed include: AsterixDB, HP Vertica, Impala, Neo4j, Redis, SparkSQL. This course provides techniques to extract value from existing untapped data sources and discovering new data sources. At the end of this course, you will be able to:\n * Recognize different data elements in your own work and in everyday life problems\n * Explain why your team needs to design a Big Data Infrastructure Plan and Information System Design\n * Identify the frequent data operations required for various types of data\n * Select a data model to suit the characteristics of your data \n * Apply techniques to handle streaming data\n * Differentiate between a traditional Database Management System and a Big Data Management System\n * Appreciate why there are so many data management systems\n * Design a big data information system for an online game company\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. At the end of the course, you will be able to: *Retrieve data from example database and big data management systems \n*Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications\n*Identify when a big data problem needs data integration\n*Execute simple big data integration and processing on Hadoop and Spark platforms\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. Want to make sense of the volumes of data you have collected?  Need to incorporate data-driven decisions into your process?  This course provides an overview of machine learning techniques to explore, analyze, and leverage data.  You will be introduced to tools and algorithms you can use to create machine learning models that learn from data, and to scale those models up to big data problems. At the end of the course, you will be able to:\n•\tDesign an approach to leverage data using the steps in the machine learning process.\n•\tApply machine learning techniques to explore and prepare data for modeling.\n•\tIdentify the type of machine learning problem in order to apply the appropriate set of techniques.\n•\tConstruct models that learn from data using widely available open source tools.\n•\tAnalyze big data problems using scalable machine learning algorithms on Spark.\n\nSoftware Requirements: \nCloudera VM, KNIME, Spark Want to understand your data network structure and how it changes under different conditions? Curious to know how to identify closely interacting clusters within a graph? Have you heard of the fast-growing area of graph analytics and want to learn more? This course gives you a broad overview of the field of graph analytics so you can learn new ways to model, store, retrieve and analyze graph-structured data. After completing this course, you will be able to model a problem into a graph database and perform analytical tasks over the graph in a scalable manner.  Better yet, you will be able to apply these techniques to understand the significance of your data sets for your own projects. Welcome to the Capstone Project for Big Data! In this culminating project, you will build a big data ecosystem using tools and methods form the earlier courses in this specialization. You will analyze a data set simulating big data generated from a large number of users who are playing our imaginary game \"Catch the Pink Flamingo\". During the five week Capstone Project, you will walk through the typical big data science steps for acquiring, exploring, preparing, analyzing, and reporting. In the first two weeks, we will introduce you to the data set and guide you through some exploratory analysis using tools such as Splunk and Open Office. Then we will move into more challenging big data problems requiring the more advanced tools you have learned including KNIME, Spark's MLLib and Gephi. Finally, during the fifth and final week, we will show you how to bring it all together to create engaging and compelling reports and slide presentations. As a result of our collaboration with Splunk, a software company focus on analyzing machine-generated big data, learners with the top projects will be eligible to present to Splunk and meet Splunk recruiters and engineering leadership.",
        "translated_description_and_objectives": "********* Do you need to understand big data and how it will impact your business? This Specialization is for you. You will gain an understanding of what insights big data can provide through hands-on experience with the tools and systems used by big data scientists and engineers. Previous programming experience is not required! You will be guided through the basics of using Hadoop with MapReduce, Spark, Pig and Hive. By following along with provided code, you will experience how one can perform predictive modeling and leverage graph analytics to model problems. This specialization will prepare you to ask the right questions about data, communicate effectively with data scientists, and do basic exploration of large, complex datasets. In the final Capstone Project, developed in partnership with data software company Splunk, you’ll apply the skills you learned to do basic analyses of big data. Interested in increasing your knowledge of the Big Data landscape?  This course is for those new to data science and interested in understanding why the Big Data Era has come to be.  It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems.  It is for those who want to start thinking about how Big Data might be useful in their business or career.  It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world! At the end of this course, you will be able to:\n\n* Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. \n\n* Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.\n\n* Get value out of Big Data by using a 5-step process to structure your analysis. \n\n* Identify what are and what are not big data problems and be able to recast big data problems as data science questions.\n\n* Provide an explanation of the architectural components and programming models used for scalable big data analysis.\n\n* Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model.\n\n* Install and run a program using Hadoop!\n\nThis course is for those new to data science.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  \n\nHardware Requirements:\n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size.  \n\nSoftware Requirements:\nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. Once you’ve identified a big data issue to analyze, how do you collect, store and organize your data using Big Data solutions?  In this course, you will experience various data genres and management tools appropriate for each.  You will be able to describe the reasons behind the evolving plethora of new big data platforms from the perspective of big data management systems and analytical tools.  Through guided hands-on tutorials, you will become familiar with techniques using real-time and semi-structured data examples.  Systems and tools discussed include: AsterixDB, HP Vertica, Impala, Neo4j, Redis, SparkSQL. This course provides techniques to extract value from existing untapped data sources and discovering new data sources. At the end of this course, you will be able to:\n * Recognize different data elements in your own work and in everyday life problems\n * Explain why your team needs to design a Big Data Infrastructure Plan and Information System Design\n * Identify the frequent data operations required for various types of data\n * Select a data model to suit the characteristics of your data \n * Apply techniques to handle streaming data\n * Differentiate between a traditional Database Management System and a Big Data Management System\n * Appreciate why there are so many data management systems\n * Design a big data information system for an online game company\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. At the end of the course, you will be able to: *Retrieve data from example database and big data management systems \n*Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications\n*Identify when a big data problem needs data integration\n*Execute simple big data integration and processing on Hadoop and Spark platforms\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+. Want to make sense of the volumes of data you have collected?  Need to incorporate data-driven decisions into your process?  This course provides an overview of machine learning techniques to explore, analyze, and leverage data.  You will be introduced to tools and algorithms you can use to create machine learning models that learn from data, and to scale those models up to big data problems. At the end of the course, you will be able to:\n•\tDesign an approach to leverage data using the steps in the machine learning process.\n•\tApply machine learning techniques to explore and prepare data for modeling.\n•\tIdentify the type of machine learning problem in order to apply the appropriate set of techniques.\n•\tConstruct models that learn from data using widely available open source tools.\n•\tAnalyze big data problems using scalable machine learning algorithms on Spark.\n\nSoftware Requirements: \nCloudera VM, KNIME, Spark Want to understand your data network structure and how it changes under different conditions? Curious to know how to identify closely interacting clusters within a graph? Have you heard of the fast-growing area of graph analytics and want to learn more? This course gives you a broad overview of the field of graph analytics so you can learn new ways to model, store, retrieve and analyze graph-structured data. After completing this course, you will be able to model a problem into a graph database and perform analytical tasks over the graph in a scalable manner.  Better yet, you will be able to apply these techniques to understand the significance of your data sets for your own projects. Welcome to the Capstone Project for Big Data! In this culminating project, you will build a big data ecosystem using tools and methods form the earlier courses in this specialization. You will analyze a data set simulating big data generated from a large number of users who are playing our imaginary game \"Catch the Pink Flamingo\". During the five week Capstone Project, you will walk through the typical big data science steps for acquiring, exploring, preparing, analyzing, and reporting. In the first two weeks, we will introduce you to the data set and guide you through some exploratory analysis using tools such as Splunk and Open Office. Then we will move into more challenging big data problems requiring the more advanced tools you have learned including KNIME, Spark's MLLib and Gephi. Finally, during the fifth and final week, we will show you how to bring it all together to create engaging and compelling reports and slide presentations. As a result of our collaboration with Splunk, a software company focus on analyzing machine-generated big data, learners with the top projects will be eligible to present to Splunk and meet Splunk recruiters and engineering leadership."
    },
    {
        "url": "https://www.coursera.org/learn/introduction-to-big-data-with-spark-hadoop",
        "original_title": "Introduction to Big Data with Spark and Hadoop",
        "translated_title": "Introduction to Big Data with Spark and Hadoop",
        "original_what_youll_learn": "Explain the impact of big data, including use cases, tools, and processing methods., Describe Apache Hadoop architecture, ecosystem, practices, and user-related applications, including Hive, HDFS, HBase, Spark, and MapReduce., Apply Spark programming basics, including parallel programming basics for DataFrames, data sets, and Spark SQL., Use Spark’s RDDs and data sets, optimize Spark SQL using Catalyst and Tungsten, and use Spark’s development and runtime environment options.",
        "translated_what_youll_learn": "Explain the impact of big data, including use cases, tools, and processing methods., Describe Apache Hadoop architecture, ecosystem, practices, and user-related applications, including Hive, HDFS, HBase, Spark, and MapReduce., Apply Spark programming basics, including parallel programming basics for DataFrames, data sets, and Spark SQL., Use Spark’s RDDs and data sets, optimize Spark SQL using Catalyst and Tungsten, and use Spark’s development and runtime environment options.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This self-paced IBM course will teach you all about big data! You will become familiar with the characteristics of big data and its application in big data analytics. You will also gain hands-on experience with big data processing tools like Apache Hadoop and Apache Spark. Bernard Marr defines big data as the digital trace that we are generating in this digital era. You will start the course by understanding what big data is and exploring how insights from big data can be harnessed for a variety of use cases. You’ll also explore how big data uses technologies like parallel processing, scaling, and data parallelism. \n\nNext, you will learn about Hadoop, an open-source framework that allows for the distributed processing of large data and its ecosystem. You will discover important applications that go hand in hand with Hadoop, like Distributed File System (HDFS), MapReduce, and HBase. You will become familiar with Hive, a data warehouse software that provides an SQL-like interface to efficiently query and manipulate large data sets.  \n\nYou’ll then gain insights into Apache Spark, an open-source processing engine that provides users with new ways to store and use big data. In this course, you will discover how to leverage Spark to deliver reliable insights. The course provides an overview of the platform, going into the components that make up Apache Spark.  \n\nYou’ll learn about DataFrames and perform basic DataFrame operations and work with SparkSQL. Explore how Spark processes and monitors the requests your application submits and how you can track work using the Spark Application UI.  \n\nThis course has several hands-on labs to help you apply and practice the concepts you learn. You will complete Hadoop and Spark labs using various tools and technologies, including Docker, Kubernetes, Python, and Jupyter Notebooks.",
        "translated_description_and_objectives": "This self-paced IBM course will teach you all about big data! You will become familiar with the characteristics of big data and its application in big data analytics. You will also gain hands-on experience with big data processing tools like Apache Hadoop and Apache Spark. Bernard Marr defines big data as the digital trace that we are generating in this digital era. You will start the course by understanding what big data is and exploring how insights from big data can be harnessed for a variety of use cases. You’ll also explore how big data uses technologies like parallel processing, scaling, and data parallelism. \n\nNext, you will learn about Hadoop, an open-source framework that allows for the distributed processing of large data and its ecosystem. You will discover important applications that go hand in hand with Hadoop, like Distributed File System (HDFS), MapReduce, and HBase. You will become familiar with Hive, a data warehouse software that provides an SQL-like interface to efficiently query and manipulate large data sets.  \n\nYou’ll then gain insights into Apache Spark, an open-source processing engine that provides users with new ways to store and use big data. In this course, you will discover how to leverage Spark to deliver reliable insights. The course provides an overview of the platform, going into the components that make up Apache Spark.  \n\nYou’ll learn about DataFrames and perform basic DataFrame operations and work with SparkSQL. Explore how Spark processes and monitors the requests your application submits and how you can track work using the Spark Application UI.  \n\nThis course has several hands-on labs to help you apply and practice the concepts you learn. You will complete Hadoop and Spark labs using various tools and technologies, including Docker, Kubernetes, Python, and Jupyter Notebooks."
    },
    {
        "url": "https://www.coursera.org/learn/big-data-introduction",
        "original_title": "Introduction to Big Data",
        "translated_title": "Introduction to Big Data",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Interested in increasing your knowledge of the Big Data landscape?  This course is for those new to data science and interested in understanding why the Big Data Era has come to be.  It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems.  It is for those who want to start thinking about how Big Data might be useful in their business or career.  It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world! At the end of this course, you will be able to:\n\n* Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. \n\n* Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.\n\n* Get value out of Big Data by using a 5-step process to structure your analysis. \n\n* Identify what are and what are not big data problems and be able to recast big data problems as data science questions.\n\n* Provide an explanation of the architectural components and programming models used for scalable big data analysis.\n\n* Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model.\n\n* Install and run a program using Hadoop!\n\nThis course is for those new to data science.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  \n\nHardware Requirements:\n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size.  \n\nSoftware Requirements:\nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.",
        "translated_description_and_objectives": "Interested in increasing your knowledge of the Big Data landscape?  This course is for those new to data science and interested in understanding why the Big Data Era has come to be.  It is for those who want to become conversant with the terminology and the core concepts behind big data problems, applications, and systems.  It is for those who want to start thinking about how Big Data might be useful in their business or career.  It provides an introduction to one of the most common frameworks, Hadoop, that has made big data analysis easier and more accessible -- increasing the potential for data to transform our world! At the end of this course, you will be able to:\n\n* Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. \n\n* Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.\n\n* Get value out of Big Data by using a 5-step process to structure your analysis. \n\n* Identify what are and what are not big data problems and be able to recast big data problems as data science questions.\n\n* Provide an explanation of the architectural components and programming models used for scalable big data analysis.\n\n* Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model.\n\n* Install and run a program using Hadoop!\n\nThis course is for those new to data science.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  \n\nHardware Requirements:\n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size.  \n\nSoftware Requirements:\nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge. Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+."
    },
    {
        "url": "https://www.coursera.org/specializations/big-data-processing-using-hadoop",
        "original_title": "Big Data Processing Using Hadoop Specialization",
        "translated_title": "Big Data Processing Using Hadoop Specialization",
        "original_what_youll_learn": "Gain expertise in Hadoop ecosystem components like HDFS, YARN, and MapReduce for big data processing and management across various tasks., Learn to set up, configure, and utilize tools like Hive, Pig, HBase, and Spark for efficient data analysis, processing, and real-time management., Develop advanced programming techniques for MapReduce, optimization methods, and parallelism strategies to handle large-scale data sets effectively., Understand the architecture and functionality of Hadoop and its components, applying them to solve complex data challenges in real-world scenarios.",
        "translated_what_youll_learn": "Gain expertise in Hadoop ecosystem components like HDFS, YARN, and MapReduce for big data processing and management across various tasks., Learn to set up, configure, and utilize tools like Hive, Pig, HBase, and Spark for efficient data analysis, processing, and real-time management., Develop advanced programming techniques for MapReduce, optimization methods, and parallelism strategies to handle large-scale data sets effectively., Understand the architecture and functionality of Hadoop and its components, applying them to solve complex data challenges in real-world scenarios.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "The specialization \"Big Data Processing Using Hadoop\" is intended for post-graduate students seeking to develop advanced skills in big data processing and management using the Hadoop ecosystem. Through four detailed courses, you will explore key technologies such as HDFS, MapReduce, and advanced data analysis tools like Hive, Pig, HBase, and Apache Spark. You’ll learn how to set up, configure, and optimize these tools to process, manage, and analyze large-scale datasets. The program covers fundamental concepts such as YARN and MapReduce architecture, and progresses to practical applications such as Hive query execution, Pig scripting, NoSQL management with HBase, and high-performance data processing with Spark. By the end of the specialization, you will be capable of designing and deploying big data solutions, optimizing workflows, and leveraging the power of Hadoop to address real-world challenges. This specialization prepares you for roles such as Data Engineer, Big Data Analyst, or Hadoop Developer, making you a highly competitive candidate in the fast-growing big data field, ready to drive innovations in industries such as data science, business analytics, and machine learning. Applied Learning Project The specialization “Big Data Processing Using Hadoop” equips postgraduate students with in-depth knowledge of big data technologies through self-reflective readings and theoretical exploration. Covering essential tools like HDFS, MapReduce, Hive, Pig, HBase, and Apache Spark, the program delves into concepts such as YARN architecture, query optimization, NoSQL data management, and high-performance computing. Learners will critically analyze the implementation of these technologies, reflecting on their applications in solving real-world big data challenges. By the end of the program, students will be prepared for roles like Data Engineer, Big Data Analyst, or Hadoop Developer, driving innovations in data science and analytics.",
        "translated_description_and_objectives": "The specialization \"Big Data Processing Using Hadoop\" is intended for post-graduate students seeking to develop advanced skills in big data processing and management using the Hadoop ecosystem. Through four detailed courses, you will explore key technologies such as HDFS, MapReduce, and advanced data analysis tools like Hive, Pig, HBase, and Apache Spark. You’ll learn how to set up, configure, and optimize these tools to process, manage, and analyze large-scale datasets. The program covers fundamental concepts such as YARN and MapReduce architecture, and progresses to practical applications such as Hive query execution, Pig scripting, NoSQL management with HBase, and high-performance data processing with Spark. By the end of the specialization, you will be capable of designing and deploying big data solutions, optimizing workflows, and leveraging the power of Hadoop to address real-world challenges. This specialization prepares you for roles such as Data Engineer, Big Data Analyst, or Hadoop Developer, making you a highly competitive candidate in the fast-growing big data field, ready to drive innovations in industries such as data science, business analytics, and machine learning. Applied Learning Project The specialization “Big Data Processing Using Hadoop” equips postgraduate students with in-depth knowledge of big data technologies through self-reflective readings and theoretical exploration. Covering essential tools like HDFS, MapReduce, Hive, Pig, HBase, and Apache Spark, the program delves into concepts such as YARN architecture, query optimization, NoSQL data management, and high-performance computing. Learners will critically analyze the implementation of these technologies, reflecting on their applications in solving real-world big data challenges. By the end of the program, students will be prepared for roles like Data Engineer, Big Data Analyst, or Hadoop Developer, driving innovations in data science and analytics."
    },
    {
        "url": "https://www.coursera.org/specializations/nosql-big-data-and-spark-foundations",
        "original_title": "NoSQL, Big Data, and Spark Foundations Specialization",
        "translated_title": "NoSQL, Big Data, and Spark Foundations Specialization",
        "original_what_youll_learn": "Work with NoSQL databases to insert, update, delete, query, index, aggregate, and shard/partition data., Develop hands-on NoSQL experience working with MongoDB, Apache Cassandra, and IBM Cloudant., Develop foundational knowledge of Big Data and gain hands-on lab experience using Apache Hadoop, MapReduce,  Apache Spark, Spark SQL, and Kubernetes., Perform Extract, Transform and Load (ETL) processing and Machine Learning model training and deployment with Apache Spark.",
        "translated_what_youll_learn": "Work with NoSQL databases to insert, update, delete, query, index, aggregate, and shard/partition data., Develop hands-on NoSQL experience working with MongoDB, Apache Cassandra, and IBM Cloudant., Develop foundational knowledge of Big Data and gain hands-on lab experience using Apache Hadoop, MapReduce,  Apache Spark, Spark SQL, and Kubernetes., Perform Extract, Transform and Load (ETL) processing and Machine Learning model training and deployment with Apache Spark.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Big Data Engineers and professionals with NoSQL skills are highly sought after in the data management industry. This Specialization is designed for those seeking to develop fundamental skills for working with Big Data, Apache Spark, and NoSQL databases. Three information-packed courses cover popular NoSQL databases like MongoDB and Apache Cassandra,  the widely used Apache Hadoop ecosystem of Big Data tools, as well as Apache Spark analytics engine for large-scale data processing. You start with an overview of various categories of NoSQL (Not only SQL) data repositories, and then work hands-on with several of them including IBM Cloudant, MonogoDB and Cassandra. You’ll perform various data management tasks, such as creating & replicating databases, inserting, updating, deleting, querying, indexing, aggregating & sharding data. Next, you’ll gain fundamental knowledge of Big Data technologies such as Hadoop, MapReduce, HDFS, Hive, and HBase, followed by a more in depth working knowledge of Apache  Spark, Spark Dataframes, Spark SQL, PySpark, the Spark Application UI, and scaling Spark with Kubernetes. In the final course, you will learn to work with Spark Structured Streaming  Spark ML - for performing Extract, Transform and Load processing (ETL) and machine learning tasks. This specialization is suitable for beginners in the fields of NoSQL and Big Data – whether you are or preparing to be a Data Engineer, Software Developer, IT Architect, Data Scientist, or IT Manager. Applied Learning Project The emphasis in this specialization is on learning by doing. As such, each course includes hands-on labs to practice & apply the NoSQL and Big Data skills you learn during lectures. In the first course, you will work hands-on with several NoSQL databases- MongoDB, Apache Cassandra, and IBM Cloudant to perform a variety of tasks: creating the database, adding documents, querying data, utilizing the HTTP API, performing Create, Read, Update & Delete (CRUD) operations, limiting & sorting records, indexing, aggregation, replication, using CQL shell, keyspace operations, & other table operations. In the next course, you’ll launch a Hadoop cluster using Docker and run Map Reduce jobs. You’ll\nexplore working with Spark using Jupyter notebooks on a Python kernel. You’ll build your Spark skills using DataFrames,  Spark SQL, and scale your jobs using Kubernetes. In the final course you will use Spark for ETL processing,  and Machine Learning model training and deployment using IBM Watson.",
        "translated_description_and_objectives": "Big Data Engineers and professionals with NoSQL skills are highly sought after in the data management industry. This Specialization is designed for those seeking to develop fundamental skills for working with Big Data, Apache Spark, and NoSQL databases. Three information-packed courses cover popular NoSQL databases like MongoDB and Apache Cassandra,  the widely used Apache Hadoop ecosystem of Big Data tools, as well as Apache Spark analytics engine for large-scale data processing. You start with an overview of various categories of NoSQL (Not only SQL) data repositories, and then work hands-on with several of them including IBM Cloudant, MonogoDB and Cassandra. You’ll perform various data management tasks, such as creating & replicating databases, inserting, updating, deleting, querying, indexing, aggregating & sharding data. Next, you’ll gain fundamental knowledge of Big Data technologies such as Hadoop, MapReduce, HDFS, Hive, and HBase, followed by a more in depth working knowledge of Apache  Spark, Spark Dataframes, Spark SQL, PySpark, the Spark Application UI, and scaling Spark with Kubernetes. In the final course, you will learn to work with Spark Structured Streaming  Spark ML - for performing Extract, Transform and Load processing (ETL) and machine learning tasks. This specialization is suitable for beginners in the fields of NoSQL and Big Data – whether you are or preparing to be a Data Engineer, Software Developer, IT Architect, Data Scientist, or IT Manager. Applied Learning Project The emphasis in this specialization is on learning by doing. As such, each course includes hands-on labs to practice & apply the NoSQL and Big Data skills you learn during lectures. In the first course, you will work hands-on with several NoSQL databases- MongoDB, Apache Cassandra, and IBM Cloudant to perform a variety of tasks: creating the database, adding documents, querying data, utilizing the HTTP API, performing Create, Read, Update & Delete (CRUD) operations, limiting & sorting records, indexing, aggregation, replication, using CQL shell, keyspace operations, & other table operations. In the next course, you’ll launch a Hadoop cluster using Docker and run Map Reduce jobs. You’ll\nexplore working with Spark using Jupyter notebooks on a Python kernel. You’ll build your Spark skills using DataFrames,  Spark SQL, and scale your jobs using Kubernetes. In the final course you will use Spark for ETL processing,  and Machine Learning model training and deployment using IBM Watson."
    },
    {
        "url": "https://www.coursera.org/professional-certificates/ibm-data-engineer",
        "original_title": "IBM Data Engineering Professional Certificate",
        "translated_title": "IBM Data Engineering Professional Certificate",
        "original_what_youll_learn": "Master the most up-to-date practical skills and knowledge data engineers use in their daily roles, Learn to create, design, & manage relational databases & apply database administration (DBA) concepts to RDBMSs such as MySQL, PostgreSQL, & IBM Db2, Develop working knowledge of NoSQL & Big Data using MongoDB, Cassandra, Cloudant, Hadoop, Apache Spark, Spark SQL, Spark ML, and Spark Streaming, Implement ETL & Data Pipelines with Bash, Airflow & Kafka; architect, populate, deploy Data Warehouses; create BI reports & interactive dashboards",
        "translated_what_youll_learn": "Master the most up-to-date practical skills and knowledge data engineers use in their daily roles, Learn to create, design, & manage relational databases & apply database administration (DBA) concepts to RDBMSs such as MySQL, PostgreSQL, & IBM Db2, Develop working knowledge of NoSQL & Big Data using MongoDB, Cassandra, Cloudant, Hadoop, Apache Spark, Spark SQL, Spark ML, and Spark Streaming, Implement ETL & Data Pipelines with Bash, Airflow & Kafka; architect, populate, deploy Data Warehouses; create BI reports & interactive dashboards",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Prepare for a career in the high-growth field of data engineering. In this program, you’ll learn in-demand skills like Python, SQL, and Databases to get job-ready in less than 5 months. Data engineering is building systems to gather data, process and organize raw data into usable information. Data engineers provide the foundational information that data scientists and business intelligence analysts use to make decisions. This program will teach you the foundational data engineering skills employers are seeking for entry level data engineering roles, including Python, one of the most widely used programming languages. You’ll also master SQL, RDBMS, ETL, Data Warehousing, NoSQL, Big Data, and Spark with hands-on labs and projects. You’ll learn to use Python programming language and Linux/UNIX shell scripts to extract, transform and load (ETL) data. You’ll work with Relational Databases (RDBMS) and query data using SQL statements and use NoSQL databases as well as unstructured data. You'll also learn how generative AI tools and techniques are used in data engineering. Upon completion, you’ll have a portfolio of projects and a Professional Certificate from IBM to showcase your expertise. You’ll also earn an IBM Digital badge and will gain access to career resources to help you in your job search, including mock interviews and resume support. This program is ACE® recommended—when you complete, you can earn up to 12 college credits. Applied Learning Project Throughout this Professional Certificate, you will complete hands-on labs and projects to help you gain practical experience with Python, SQL, relational databases, NoSQL databases, Apache Spark, building data pipelines, managing databases, and working with data warehouses. Projects: Design a relational database to help a coffee franchise improve operations. Use SQL to query census, crime, and school demographic data sets. Write a Bash shell script on Linux that backups changed files. Set up, test, and optimize a data platform that contains MySQL, PostgreSQL, and IBM Db2 databases. Analyze road traffic data to perform ETL and create a pipeline using Airflow and Kafka. Design and implement a data warehouse for a solid-waste management company. Move, query, and analyze data in MongoDB, Cassandra, and Cloudant NoSQL databases. Train a machine learning model by creating an Apache Spark application. Design, deploy, and manage an end-to-end data engineering platform.",
        "translated_description_and_objectives": "Prepare for a career in the high-growth field of data engineering. In this program, you’ll learn in-demand skills like Python, SQL, and Databases to get job-ready in less than 5 months. Data engineering is building systems to gather data, process and organize raw data into usable information. Data engineers provide the foundational information that data scientists and business intelligence analysts use to make decisions. This program will teach you the foundational data engineering skills employers are seeking for entry level data engineering roles, including Python, one of the most widely used programming languages. You’ll also master SQL, RDBMS, ETL, Data Warehousing, NoSQL, Big Data, and Spark with hands-on labs and projects. You’ll learn to use Python programming language and Linux/UNIX shell scripts to extract, transform and load (ETL) data. You’ll work with Relational Databases (RDBMS) and query data using SQL statements and use NoSQL databases as well as unstructured data. You'll also learn how generative AI tools and techniques are used in data engineering. Upon completion, you’ll have a portfolio of projects and a Professional Certificate from IBM to showcase your expertise. You’ll also earn an IBM Digital badge and will gain access to career resources to help you in your job search, including mock interviews and resume support. This program is ACE® recommended—when you complete, you can earn up to 12 college credits. Applied Learning Project Throughout this Professional Certificate, you will complete hands-on labs and projects to help you gain practical experience with Python, SQL, relational databases, NoSQL databases, Apache Spark, building data pipelines, managing databases, and working with data warehouses. Projects: Design a relational database to help a coffee franchise improve operations. Use SQL to query census, crime, and school demographic data sets. Write a Bash shell script on Linux that backups changed files. Set up, test, and optimize a data platform that contains MySQL, PostgreSQL, and IBM Db2 databases. Analyze road traffic data to perform ETL and create a pipeline using Airflow and Kafka. Design and implement a data warehouse for a solid-waste management company. Move, query, and analyze data in MongoDB, Cassandra, and Cloudant NoSQL databases. Train a machine learning model by creating an Apache Spark application. Design, deploy, and manage an end-to-end data engineering platform."
    },
    {
        "url": "https://www.coursera.org/learn/introduction-to-data-analytics",
        "original_title": "Introduction to Data Analytics",
        "translated_title": "Introduction to Data Analytics",
        "original_what_youll_learn": "Explain what Data Analytics is and the key steps in the Data Analytics process, Differentiate between different data roles such as Data Engineer, Data Analyst, Data Scientist, Business Analyst, and Business Intelligence Analyst, Describe the different types of data structures, file formats, and sources of data, Describe the data analysis process involving collecting, wrangling, mining, and visualizing data",
        "translated_what_youll_learn": "Explain what Data Analytics is and the key steps in the Data Analytics process, Differentiate between different data roles such as Data Engineer, Data Analyst, Data Scientist, Business Analyst, and Business Intelligence Analyst, Describe the different types of data structures, file formats, and sources of data, Describe the data analysis process involving collecting, wrangling, mining, and visualizing data",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Ready to start a career in Data Analysis but don’t know where to begin? This course presents you with a gentle introduction to Data Analysis, the role of a Data Analyst, and the tools used in this job. You will learn about the skills and responsibilities of a data analyst and hear from several data experts sharing their tips & advice to start a career. This course will help you to differentiate between the roles of Data Analysts, Data Scientists, and Data Engineers. You will familiarize yourself with the data ecosystem, alongside Databases, Data Warehouses, Data Marts, Data Lakes and Data Pipelines. Continue this exciting journey and discover Big Data platforms such as Hadoop, Hive, and Spark.  \n\nBy the end of this course you’ll be able to understand the fundamentals of the data analysis process including gathering, cleaning, analyzing and sharing data and communicating your insights with the use of visualizations and dashboard tools. \n\nThis all comes together in the final project where it will test your knowledge of the course material, and provide a real-world scenario of data analysis tasks.  \n\nThis course does not require any prior data analysis, spreadsheet, or computer science experience.",
        "translated_description_and_objectives": "Ready to start a career in Data Analysis but don’t know where to begin? This course presents you with a gentle introduction to Data Analysis, the role of a Data Analyst, and the tools used in this job. You will learn about the skills and responsibilities of a data analyst and hear from several data experts sharing their tips & advice to start a career. This course will help you to differentiate between the roles of Data Analysts, Data Scientists, and Data Engineers. You will familiarize yourself with the data ecosystem, alongside Databases, Data Warehouses, Data Marts, Data Lakes and Data Pipelines. Continue this exciting journey and discover Big Data platforms such as Hadoop, Hive, and Spark.  \n\nBy the end of this course you’ll be able to understand the fundamentals of the data analysis process including gathering, cleaning, analyzing and sharing data and communicating your insights with the use of visualizations and dashboard tools. \n\nThis all comes together in the final project where it will test your knowledge of the course material, and provide a real-world scenario of data analysis tasks.  \n\nThis course does not require any prior data analysis, spreadsheet, or computer science experience."
    },
    {
        "url": "https://www.coursera.org/specializations/data-analytics-and-big-data",
        "original_title": "Data Analytics and Big Data Specialization",
        "translated_title": "Data Analytics and Big Data Specialization",
        "original_what_youll_learn": "Design and implement relational databases using SQL, Manage and process big data using technologies like Hadoop and Spark, Perform data analysis and preprocessing to extract valuable insights",
        "translated_what_youll_learn": "Design and implement relational databases using SQL, Manage and process big data using technologies like Hadoop and Spark, Perform data analysis and preprocessing to extract valuable insights",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This specialization provides a comprehensive understanding of relational databases, data preprocessing, and big data technologies. Learners will explore database design, implementation, and applications, along with data preparation and analysis techniques. The specialization is designed to help data scientists and analysts manage and analyze large datasets efficiently using industry-standard tools. Applied Learning Project Learners will engage in projects that involve designing and implementing relational databases, preprocessing data for analysis, and applying big data technologies to solve real-world data challenges in the courses in this specialization. However, there is no required capstone project for this specialization.",
        "translated_description_and_objectives": "This specialization provides a comprehensive understanding of relational databases, data preprocessing, and big data technologies. Learners will explore database design, implementation, and applications, along with data preparation and analysis techniques. The specialization is designed to help data scientists and analysts manage and analyze large datasets efficiently using industry-standard tools. Applied Learning Project Learners will engage in projects that involve designing and implementing relational databases, preprocessing data for analysis, and applying big data technologies to solve real-world data challenges in the courses in this specialization. However, there is no required capstone project for this specialization."
    },
    {
        "url": "https://www.coursera.org/specializations/gcp-data-machine-learning",
        "original_title": "Data Engineering, Big Data, and Machine Learning on GCP Specialization",
        "translated_title": "Data Engineering, Big Data, and Machine Learning on GCP Specialization",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "87% of Google Cloud certified users feel more confident in their cloud skills. This program provides the skills you need to advance your career and provides training to support your preparation for the industry-recognized Google Cloud Professional Data EngineerOpens in a new tab certification. Here's what you have to do 1) Complete the Coursera Data Engineering Professional Certificate 2) Review other recommended resources for the Google Cloud Professional Data Engineer certificationOpens in a new tab exam 3) Review the Professional Data Engineer exam guideOpens in a new tab 4) Complete Professional Data Engineer Opens in a new tabsample questions 5) RegisterOpens in a new tab for the Google Cloud certification exam (remotely or at a test center) Applied Learning Project This professional certificate incorporates hands-on labs using Qwiklabs platform.These hands on components will let you apply the skills you learn. Projects incorporate Google Cloud products used within Qwiklabs. You will gain practical hands-on experience with the concepts explained throughout the modules. Applied Learning Project This Specialization incorporates hands-on labs using our Qwiklabs platform. These hands on components will let you apply the skills you learn in the video lectures. Projects will incorporate topics such as BigQuery, which are used and configured within Qwiklabs. You can expect to gain practical hands-on experience with the concepts explained throughout the modules.",
        "translated_description_and_objectives": "87% of Google Cloud certified users feel more confident in their cloud skills. This program provides the skills you need to advance your career and provides training to support your preparation for the industry-recognized Google Cloud Professional Data EngineerOpens in a new tab certification. Here's what you have to do 1) Complete the Coursera Data Engineering Professional Certificate 2) Review other recommended resources for the Google Cloud Professional Data Engineer certificationOpens in a new tab exam 3) Review the Professional Data Engineer exam guideOpens in a new tab 4) Complete Professional Data Engineer Opens in a new tabsample questions 5) RegisterOpens in a new tab for the Google Cloud certification exam (remotely or at a test center) Applied Learning Project This professional certificate incorporates hands-on labs using Qwiklabs platform.These hands on components will let you apply the skills you learn. Projects incorporate Google Cloud products used within Qwiklabs. You will gain practical hands-on experience with the concepts explained throughout the modules. Applied Learning Project This Specialization incorporates hands-on labs using our Qwiklabs platform. These hands on components will let you apply the skills you learn in the video lectures. Projects will incorporate topics such as BigQuery, which are used and configured within Qwiklabs. You can expect to gain practical hands-on experience with the concepts explained throughout the modules."
    },
    {
        "url": "https://www.coursera.org/specializations/software-architecture-big-data",
        "original_title": "Software Architecture for Big Data Specialization",
        "translated_title": "Software Architecture for Big Data Specialization",
        "original_what_youll_learn": "Practice software engineering fundamentals; test first development, refactoring, continuous integration, and continuous delivery., Architect and create a big data or distributed system using rest collaboration, event collaboration, and batch processing., Create4 a performant, scalable distributed system that handles big data.",
        "translated_what_youll_learn": "Practice software engineering fundamentals; test first development, refactoring, continuous integration, and continuous delivery., Architect and create a big data or distributed system using rest collaboration, event collaboration, and batch processing., Create4 a performant, scalable distributed system that handles big data.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This specialization is for software engineers interested in the principles of building and architecting large software systems that use big data. Through three courses you will learn about how to build and architect performant distributed systems from industry experts at Initial Capacity. This specialization can be taken for academic credit as part of CU Boulder’s MS in Data Science or MS in Computer Science degrees offered on the Coursera platform. These fully accredited graduate degrees offer targeted courses, short 8-week sessions, and pay-as-you-go tuition. Admission is based on performance in three preliminary courses, not academic history. CU degrees on Coursera are ideal for recent graduates or working professionals. Learn more: MS in Data Science: https://www.coursera.org/degrees/master-of-science-data-science-boulder MS in Computer Science: https://coursera.org/degrees/ms-computer-science-boulder Applied Learning Project The first course will introduce you to software architecture and design concepts necessary to build and scale large, data intensive, distributed systems. Starting with software engineering best practices and loosely coupled, highly cohesive data microservices, the course will take you through the evolution of a distributed system over time. In the second course you will then learn what is needed to take big data to production, transforming big data prototypes into high quality tested production software. You will measure the performance characteristics of distributed systems, identify trouble areas, and implement scalable solutions to improve performance The specialization concludes with a projects course in which you will use learnings from the first and second courses to build a production-ready distributed system. As you progress, your instructors will guide you around common pitfalls and share their experiences in building big data systems.",
        "translated_description_and_objectives": "This specialization is for software engineers interested in the principles of building and architecting large software systems that use big data. Through three courses you will learn about how to build and architect performant distributed systems from industry experts at Initial Capacity. This specialization can be taken for academic credit as part of CU Boulder’s MS in Data Science or MS in Computer Science degrees offered on the Coursera platform. These fully accredited graduate degrees offer targeted courses, short 8-week sessions, and pay-as-you-go tuition. Admission is based on performance in three preliminary courses, not academic history. CU degrees on Coursera are ideal for recent graduates or working professionals. Learn more: MS in Data Science: https://www.coursera.org/degrees/master-of-science-data-science-boulder MS in Computer Science: https://coursera.org/degrees/ms-computer-science-boulder Applied Learning Project The first course will introduce you to software architecture and design concepts necessary to build and scale large, data intensive, distributed systems. Starting with software engineering best practices and loosely coupled, highly cohesive data microservices, the course will take you through the evolution of a distributed system over time. In the second course you will then learn what is needed to take big data to production, transforming big data prototypes into high quality tested production software. You will measure the performance characteristics of distributed systems, identify trouble areas, and implement scalable solutions to improve performance The specialization concludes with a projects course in which you will use learnings from the first and second courses to build a production-ready distributed system. As you progress, your instructors will guide you around common pitfalls and share their experiences in building big data systems."
    },
    {
        "url": "https://www.coursera.org/specializations/cloudera-big-data-analysis-sql",
        "original_title": "Modern Big Data Analysis with SQL Specialization",
        "translated_title": "Modern Big Data Analysis with SQL Specialization",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This Specialization teaches the essential skills for working with large-scale data using SQL. Maybe you are new to SQL and you want to learn the basics. Or maybe you already have some experience using SQL to query smaller-scale data with relational databases. Either way, if you are interested in gaining the skills necessary to query big data with modern distributed SQL engines, this Specialization is for you. Most courses that teach SQL focus on traditional relational databases, but today, more and more of the data that’s being generated is too big to be stored there, and it’s growing too quickly to be efficiently stored in commercial data warehouses. Instead, it’s increasingly stored in distributed clusters and cloud storage. These data stores are cost-efficient and infinitely scalable. To query these huge datasets in clusters and cloud storage, you need a newer breed of SQL engine: distributed query engines, like Hive, Impala, Presto, and Drill. These are open source SQL engines capable of querying enormous datasets. This Specialization focuses on Hive and Impala, the most widely deployed of these query engines. This Specialization is designed to provide excellent preparation for the Cloudera Certified Associate (CCA) Data AnalystOpens in a new tab certification exam. You can earn this certification credential by taking a hands-on practical exam using the same SQL engines that this Specialization teaches—Hive and Impala. Applied Learning Project Each course in this Specialization includes a hands-on, peer-graded assignment. To earn the Specialization Certificate, you must successfully complete the hands-on, peer-graded assignment in each course. For this Specialization, there is not a separate Capstone Project like there is in some other Coursera Specializations.",
        "translated_description_and_objectives": "This Specialization teaches the essential skills for working with large-scale data using SQL. Maybe you are new to SQL and you want to learn the basics. Or maybe you already have some experience using SQL to query smaller-scale data with relational databases. Either way, if you are interested in gaining the skills necessary to query big data with modern distributed SQL engines, this Specialization is for you. Most courses that teach SQL focus on traditional relational databases, but today, more and more of the data that’s being generated is too big to be stored there, and it’s growing too quickly to be efficiently stored in commercial data warehouses. Instead, it’s increasingly stored in distributed clusters and cloud storage. These data stores are cost-efficient and infinitely scalable. To query these huge datasets in clusters and cloud storage, you need a newer breed of SQL engine: distributed query engines, like Hive, Impala, Presto, and Drill. These are open source SQL engines capable of querying enormous datasets. This Specialization focuses on Hive and Impala, the most widely deployed of these query engines. This Specialization is designed to provide excellent preparation for the Cloudera Certified Associate (CCA) Data AnalystOpens in a new tab certification exam. You can earn this certification credential by taking a hands-on practical exam using the same SQL engines that this Specialization teaches—Hive and Impala. Applied Learning Project Each course in this Specialization includes a hands-on, peer-graded assignment. To earn the Specialization Certificate, you must successfully complete the hands-on, peer-graded assignment in each course. For this Specialization, there is not a separate Capstone Project like there is in some other Coursera Specializations."
    },
    {
        "url": "https://www.coursera.org/learn/big-data-emerging-technologies",
        "original_title": "Big Data Emerging Technologies",
        "translated_title": "Big Data Emerging Technologies",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Every time you use Google to search something, every time you use Facebook, Twitter, Instagram or any other SNS (Social Network Service), and every time you buy from a recommended list of products on Amazon.com you are using a big data system. In addition, big data technology supports your smartphone, smartwatch, Alexa, Siri, and automobile (if it is a newer model) every day. The top companies in the world are currently using big data technology, and every company is in need of advanced big data technology support. Simply put, big data technology is not an option for your company, it is a necessity for survival and growth. So now is the right time to learn what big data is and how to use it in advantage of your company. This 6 module course first focuses on the world’s industry market share rankings of big data hardware, software, and professional services, and then covers the world’s top big data product line and service types of the major big data companies. Then the lectures focused on how big data analysis is possible based on the world’s most popular three big data technologies Hadoop, Spark, and Storm. The last part focuses on providing experience on one of the most famous and widely used big data statistical analysis systems in the world, the IBM SPSS Statistics. This course was designed to prepare you to be more successful in businesses strategic planning in the upcoming big data era. Welcome to the amazing Big Data world!",
        "translated_description_and_objectives": "Every time you use Google to search something, every time you use Facebook, Twitter, Instagram or any other SNS (Social Network Service), and every time you buy from a recommended list of products on Amazon.com you are using a big data system. In addition, big data technology supports your smartphone, smartwatch, Alexa, Siri, and automobile (if it is a newer model) every day. The top companies in the world are currently using big data technology, and every company is in need of advanced big data technology support. Simply put, big data technology is not an option for your company, it is a necessity for survival and growth. So now is the right time to learn what big data is and how to use it in advantage of your company. This 6 module course first focuses on the world’s industry market share rankings of big data hardware, software, and professional services, and then covers the world’s top big data product line and service types of the major big data companies. Then the lectures focused on how big data analysis is possible based on the world’s most popular three big data technologies Hadoop, Spark, and Storm. The last part focuses on providing experience on one of the most famous and widely used big data statistical analysis systems in the world, the IBM SPSS Statistics. This course was designed to prepare you to be more successful in businesses strategic planning in the upcoming big data era. Welcome to the amazing Big Data world!"
    },
    {
        "url": "https://www.coursera.org/learn/impacto-datos-masivos",
        "original_title": "Big Data: el impacto de los datos masivos en la sociedad actual",
        "translated_title": "Big Data: The impact of mass data on today's society",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "La digitalización, la informática e Internet han producido lo que se puede denominar una revolución en la acumulación y utilización de datos. Podemos almacenar y conservar más datos que nunca antes en la historia. Podemos estudiarlos y analizarlos para tomar decisiones y mejorar procesos. Esta nueva capacidad tiene un enorme impacto en todos los ámbitos de la vida social. A lo largo de este curso:\n\n•\tConoceremos qué es el Big Data y cuáles son sus características fundamentales\n•\tExploraremos el crecimiento continuo de datos, analizaremos el impacto potencial en muchos campos de la actividad humana y nos preguntaremos por los retos y desafíos que suponen en todos los órdenes de la vida social. \n•\tConoceremos las características de cada una de las fases del procesamiento Big Data, adquiriendo un lenguaje adecuado para la descripción de los procesos. Dispondremos así de una visión de conjunto sobre sistema de tratamiento de grandes datos en la actualidad.\n•\tConoceremos las principales áreas de aplicación de los datos masivos. Qué tipos de transformaciones están imponiendo en la organización del trabajo y en la gestión. Qué desafíos imponen en la gobernanza, la economía y el trabajo. Qué mejoras introducen y qué riesgos representan.\n•\tEstudiaremos las principales tecnologías e infraestructuras para el almacenamiento y procesado de grandes volúmenes de datos.",
        "translated_description_and_objectives": "Digitization, computer science and the Internet have produced what can be called a revolution in the accumulation and use of data. We can store and keep more data than ever before in history. We can study them and analyze them to make decisions and improve processes. This new capacity has a huge impact in all areas of social life. Throughout this course:\n\n• We will know what the Big Data is and what are its fundamental characteristics\n• We will explore the continuous growth of data, we will analyze the potential impact on many fields of human activity and ask ourselves about the challenges and challenges that suppose in all the orders of social life. \n• We will know the characteristics of each of the Phases of Big Data processing, acquiring adequate language for the description of the processes. We will have an overall vision on large data processing system today.\n• We will know the main areas of application of mass data. What types of transformations are imposing on the organization of work and management. What challenges impose on governance, economy and work. What improvements introduce and what risks do they represent.\n• We will study the main technologies and infrastructure for the storage and processing of large volumes of data."
    },
    {
        "url": "https://www.coursera.org/learn/big-data-analytics-1",
        "original_title": "Big Data Analytics",
        "translated_title": "Big data analytics",
        "original_what_youll_learn": "Gain a deep understanding of Hadoop and Spark ecosystems for managing big data.\nBecome familiar with tools like Hive and Pig to query large datasets.",
        "translated_what_youll_learn": "Gain a deep understanding of Hadoop and Spark ecosystems for managing big data.\nBecome familiar with tools like Hive and Pig to query large datasets.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "The Big Data Analytics course offers a deep dive into the technologies, tools, and techniques used to process and analyze large-scale data. Learners will explore the Hadoop and Spark ecosystems, gaining hands-on experience with essential components such as Hadoop Distributed File System (HDFS), MapReduce, Pig, and Hive. The course also covers both relational (SQL) and nonrelational (NoSQL) databases, helping learners understand the appropriate contexts for each type of data storage. A significant focus is placed on Apache Spark, known for its high-speed, in-memory data processing capabilities, which is vital for handling big data applications. Learners will also work through real-world exercises, including implementing and deploying a machine learning application that processes streaming data on the cloud.\n\nDesigned for professionals with a background in predictive analytics, basic SQL, and Python programming, this course equips learners with the practical skills to manage data characterized by high volume, velocity, and variety. By the end of the course, participants will be able to derive actionable insights from big data and apply them in business contexts, contributing to improved decision-making and competitive advantage in data-driven environments.",
        "translated_description_and_objectives": "The Big Data Analytics course offers a deep dive into the technologies, tools, and techniques used to process and analyze large-scale data. Learners will explore the Hadoop and Spark ecosystems, gaining hands-on experience with essential components such as Hadoop Distributed File System (HDFS), MapReduce, Pig, and Hive. The course also covers both relational (SQL) and nonrelational (NoSQL) databases, helping learners understand the appropriate contexts for each type of data storage. A significant focus is placed on Apache Spark, known for its high-speed, in-memory data processing capabilities, which is vital for handling big data applications. Learners will also work through real-world exercises, including implementing and deploying a machine learning application that processes streaming data on the cloud.\n\nDesigned for professionals with a background in predictive analytics, basic SQL, and Python programming, this course equips learners with the practical skills to manage data characterized by high volume, velocity, and variety. By the end of the course, participants will be able to derive actionable insights from big data and apply them in business contexts, contributing to improved decision-making and competitive advantage in data-driven environments."
    },
    {
        "url": "https://www.coursera.org/learn/illinois-tech-big-data-technologies",
        "original_title": "Big Data Technologies",
        "translated_title": "Big Data Technologies",
        "original_what_youll_learn": "Understanding and identifying  use cases and domains of Big Data problems, Selecting and implementing technical solutions involving Big Data systems, Develop and use various open source software systems (Apache) in the Big Data tech stack, Operate and run various cloud computing software services (AWS) in the Big Data infrastructure space",
        "translated_what_youll_learn": "Understanding and identifying  use cases and domains of Big Data problems, Selecting and implementing technical solutions involving Big Data systems, Develop and use various open source software systems (Apache) in the Big Data tech stack, Operate and run various cloud computing software services (AWS) in the Big Data infrastructure space",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Big data is the area of informatics focusing on datasets whose size is beyond the ability of typical database and other software tools to capture, store, analyze and manage. This course provides a rapid immersion into the area of big data and the technologies which have recently emerged to manage it. We start with an introduction to the characteristics of big data and an overview of the associated technology landscape and continue with an in depth exploration of Hadoop, the leading open source framework for big data processing. Here the focus is on the most important Hadoop components such as Hive, Pig, stream processing and Spark as well as architectural patterns for applying these components. We continue with an exploration of the range of specialized (NoSQL) database systems architected to address the challenges of managing large volumes of data.\n\nOverall the objective is to develop a sense of how to make sound decisions in the adoption and use of these technologies as well as economically deploy them on modern cloud computing infrastructure.",
        "translated_description_and_objectives": "Big data is the area of informatics focusing on datasets whose size is beyond the ability of typical database and other software tools to capture, store, analyze and manage. This course provides a rapid immersion into the area of big data and the technologies which have recently emerged to manage it. We start with an introduction to the characteristics of big data and an overview of the associated technology landscape and continue with an in depth exploration of Hadoop, the leading open source framework for big data processing. Here the focus is on the most important Hadoop components such as Hive, Pig, stream processing and Spark as well as architectural patterns for applying these components. We continue with an exploration of the range of specialized (NoSQL) database systems architected to address the challenges of managing large volumes of data.\n\nOverall the objective is to develop a sense of how to make sound decisions in the adoption and use of these technologies as well as economically deploy them on modern cloud computing infrastructure."
    },
    {
        "url": "https://www.coursera.org/specializations/gcp-data-machine-learning-es",
        "original_title": "Data Engineering, Big Data and ML on Google Cloud en Español Specialization",
        "translated_title": "Data Engineering, Big Data and ML on Google Cloud en Español Specialization",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Especialización acelerada en línea de cinco semanas de duración, donde los participantes reciben una introducción práctica en el diseño y compilación de sistemas de procesamiento de datos en Google Cloud Platform. Usando una combinación de presentaciones, demostraciones y labs prácticos, los participantes aprenderán a diseñar sistemas de procesamiento de datos, compilar canalizaciones de datos de extremo a extremo, analizar datos y llevar a efecto funciones de aprendizaje automático.  Este curso le enseñará las siguientes habilidades: • Diseñar y crear sistemas de procesamiento de datos en Google Cloud Platform • Aprovechar los datos no estructurados mediante Spark y las API de AA en Cloud Dataproc • Procesar los datos por lotes y de transmisión mediante la implementación de canalizaciones de datos de ajuste de escala automático en Cloud Dataflow • Generar estadísticas empresariales a partir de conjuntos de datos muy grandes mediante Google BigQuery • Entrenar, evaluar y predecir por medio de los modelos de aprendizaje automático con Tensorflow y Cloud ML • Extraer estadísticas al instante a partir de los datos de transmisión Este curso está dirigido a desarrolladores experimentados responsables de la administración de transformaciones de macrodatos. >>> Al inscribirse en esta especialización acepta los Términos de Servicio de Qwiklabs según lo establecido en las Preguntas Frecuentes, disponibles en el apartado: https://qwiklabs.com/terms_of_serviceOpens in a new tab <<< Applied Learning Project Esta especialización incluye labs prácticos. Deberá tener una cuenta de Google (puede usar una de Gmail) y registrarse en una cuenta de prueba gratuita de Google Cloud Platform,. La prueba gratuita tiene un límite de 12 meses o $300 dólares de crédito, lo que termine primero. Es por ello que nuestra especialización está diseñada para completarse en cuatro semanas. Estos componentes prácticos le permitirán aplicar las habilidades que adquiera en las clases en video. Los proyectos incorporarán temas como Google BigQuery, que se usan y configuran en Codelabs. Además, adquirirá experiencia práctica con los conceptos que se explican en todos los módulos.",
        "translated_description_and_objectives": "Accelerate online specialization of five weeks, where participants receive a practical introduction in the design and compilation of data processing systems in Google Cloud Platform. Using a combination of presentations, demonstrations and practical labs, participants will learn to design data processing systems, compile end -to -end data pipes, analyze data and carry out automatic learning functions.  This course will teach you the following skills: • Design and create data processing systems on Google Cloud Platform • Take advantage of unstructured data through SPARK and AA API in Cloud Dataproc • Process data by lots and transmission by implementing automatic scale setting data setting in Cloud DataFlow • Generate business statistics from very large data Google Bigquery • Train, evaluate and predict through automatic learning models with tensorflow and cloud ML • Extract statistics instantly from the transmission data this course is aimed at experienced developers responsible for the administration of macrodatos transformations. >>> When registering in this specialization, it accepts the terms of Qwiklabs service as established in frequent questions, available in the section: https://qwiklabs.com/terms_of_f_Servicepens in a new tab <<< Applied Learning Project This specialization includes practical labs. You must have a Google account (you can use a Gmail) and register on a free Google Cloud Platform,. The free test has a limit of 12 months or $ 300 dollars of credit, which ends first. That is why our specialization is designed to complete in four weeks. These practical components will allow you to apply the skills you acquire in video classes. Projects will incorporate topics such as Google Bigquery, which are used and configured in Codeabs. In addition, it will acquire practical experience with the concepts that are explained in all modules."
    },
    {
        "url": "https://www.coursera.org/learn/big-data-analysis-deep-dive",
        "original_title": "Big Data Analysis Deep Dive",
        "translated_title": "Big Data Analysis Deep Dive",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "The job market for architects, engineers, and analytics professionals with Big Data expertise continues to increase. The Academy’s Big Data Career path focuses on the fundamental tools and techniques needed to pursue a career in Big Data. This course includes: data processing with python, writing and reading SQL queries, transmitting data with MaxCompute, analyzing data with Quick BI, using Hive, Hadoop, and spark on E-MapReduce, and how to visualize data with data dashboards. \n\nWork through our course material, learn different aspects of the Big Data field, and get certified as a Big Data Professional!",
        "translated_description_and_objectives": "The job market for architects, engineers, and analytics professionals with Big Data expertise continues to increase. The Academy’s Big Data Career path focuses on the fundamental tools and techniques needed to pursue a career in Big Data. This course includes: data processing with python, writing and reading SQL queries, transmitting data with MaxCompute, analyzing data with Quick BI, using Hive, Hadoop, and spark on E-MapReduce, and how to visualize data with data dashboards. \n\nWork through our course material, learn different aspects of the Big Data field, and get certified as a Big Data Professional!"
    },
    {
        "url": "https://www.coursera.org/specializations/python-bash-sql-data-engineering-duke",
        "original_title": "Python, Bash and SQL Essentials for Data Engineering Specialization",
        "translated_title": "Python, Bash and SQL Essentials for Data Engineering Specialization",
        "original_what_youll_learn": "Develop data engineering solutions with a minimal and essential subset of the Python language and the Linux environment, Design scripts to connect and query a SQL database using Python, Use a scraping library in Python to read, identify and extract data from websites",
        "translated_what_youll_learn": "Develop data engineering solutions with a minimal and essential subset of the Python language and the Linux environment, Design scripts to connect and query a SQL database using Python, Use a scraping library in Python to read, identify and extract data from websites",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "If you are interested in developing the skills needed to be a data engineer, the Python, Bash and SQL Essentials for Data Engineering Specialization is a great place to start. We live in a world that is driven by big data - from what we search online to the route we take to our favorite restaurant, and everything in between. Businesses and organizations use this data to make decisions that impact the ways in which we navigate our lives. How do engineers collect this data? How can this data be organized so that it can be appropriately analyzed? A data engineer is specialized in this initial step of accessing, cleaning and managing big data. Data engineers today need a solid foundation in a few essential areas: Python, Bash and SQL. In Python, Bash and SQL Essentials for Data Engineering, we provide a nuts and bolts overview of these fundamental skills needed for entering the world of data engineering. Led by three professional data engineers, this Specialization will provide quick and accessible ways to learn data engineering strategies, give you a chance to practice what you’ve learned in integrated lab exercises, and then immediately apply these techniques in your professional or academic life. Applied Learning Project Each course includes integrated lab exercises using Visual Studio Code or Jupyter notebooks that give you an opportunity to practice the Python, Bash and SQL skills with real-world applications covered in each course. For each data engineering solution that you explore, you are also encouraged to create a demo video and GitHub repository of code that can be showcased in your digital portfolio for employers. By the end of this Specialization, you will have the foundational skills necessary to begin tackling more complex data engineering solutions.",
        "translated_description_and_objectives": "If you are interested in developing the skills needed to be a data engineer, the Python, Bash and SQL Essentials for Data Engineering Specialization is a great place to start. We live in a world that is driven by big data - from what we search online to the route we take to our favorite restaurant, and everything in between. Businesses and organizations use this data to make decisions that impact the ways in which we navigate our lives. How do engineers collect this data? How can this data be organized so that it can be appropriately analyzed? A data engineer is specialized in this initial step of accessing, cleaning and managing big data. Data engineers today need a solid foundation in a few essential areas: Python, Bash and SQL. In Python, Bash and SQL Essentials for Data Engineering, we provide a nuts and bolts overview of these fundamental skills needed for entering the world of data engineering. Led by three professional data engineers, this Specialization will provide quick and accessible ways to learn data engineering strategies, give you a chance to practice what you’ve learned in integrated lab exercises, and then immediately apply these techniques in your professional or academic life. Applied Learning Project Each course includes integrated lab exercises using Visual Studio Code or Jupyter notebooks that give you an opportunity to practice the Python, Bash and SQL skills with real-world applications covered in each course. For each data engineering solution that you explore, you are also encouraged to create a demo video and GitHub repository of code that can be showcased in your digital portfolio for employers. By the end of this Specialization, you will have the foundational skills necessary to begin tackling more complex data engineering solutions."
    },
    {
        "url": "https://www.coursera.org/specializations/python-data-engineering",
        "original_title": "Applied Python Data Engineering Specialization",
        "translated_title": "Applied Python Data Engineering Specialization",
        "original_what_youll_learn": "Create scalable big data pipelines (Hadoop, Spark, Snowflake, Databricks) for efficient data handling., Build machine learning workflows (PySpark, MLFlow) on Databricks for seamless model development and deployment., Implement DataOps/DevOps to streamline data engineering processes., Formulate and communicate data-driven insights and narratives through impactful visualizations with Python and data storytelling",
        "translated_what_youll_learn": "Create scalable big data pipelines (Hadoop, Spark, Snowflake, Databricks) for efficient data handling., Build machine learning workflows (PySpark, MLFlow) on Databricks for seamless model development and deployment., Implement DataOps/DevOps to streamline data engineering processes., Formulate and communicate data-driven insights and narratives through impactful visualizations with Python and data storytelling",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Learn how to use data engineering to leverage big data for business strategy, data analysis, or machine learning and AI. By completing this course series, you'll empower yourself with the knowledge and proficiency required to build efficient data pipelines, manage cutting-edge platforms like Hadoop, Spark, Snowflake, Databricks, and Kubernetes, and tell stories with data through visualization. You will delve into foundational big data concepts, distributed computing with Spark, Snowflake’s architecture, Databricks’ machine learning capabilities, Python techniques for data visualization, and critical methodologies like DataOps. This course series is designed for software engineers, developers, researchers, and data scientists who want to strengthen their specialization in data science or machine learning, as well as for professionals who are interested in pursuing a career as a data-focused software engineer, data scientist, or a data engineer working in cloud, machine learning, business intelligence, or other field. Applied Learning Project The Specialization features a capstone project focused on using Databricks’ API to replicate an existing project. This provides hands-on experience working with Databricks to build a portfolio-ready data solution. You will apply Python to a variety of data engineering tasks.",
        "translated_description_and_objectives": "Learn how to use data engineering to leverage big data for business strategy, data analysis, or machine learning and AI. By completing this course series, you'll empower yourself with the knowledge and proficiency required to build efficient data pipelines, manage cutting-edge platforms like Hadoop, Spark, Snowflake, Databricks, and Kubernetes, and tell stories with data through visualization. You will delve into foundational big data concepts, distributed computing with Spark, Snowflake’s architecture, Databricks’ machine learning capabilities, Python techniques for data visualization, and critical methodologies like DataOps. This course series is designed for software engineers, developers, researchers, and data scientists who want to strengthen their specialization in data science or machine learning, as well as for professionals who are interested in pursuing a career as a data-focused software engineer, data scientist, or a data engineer working in cloud, machine learning, business intelligence, or other field. Applied Learning Project The Specialization features a capstone project focused on using Databricks’ API to replicate an existing project. This provides hands-on experience working with Databricks to build a portfolio-ready data solution. You will apply Python to a variety of data engineering tasks."
    },
    {
        "url": "https://www.coursera.org/learn/bigdataanalysis",
        "original_title": "大數據分析：商業應用與策略管理 (Big Data Analytics: Business Applications and Strategic Decisions)",
        "translated_title": "大數據分析：商業應用與策略管理 (Big Data Analytics: Business Applications and Strategic Decisions)",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "本課程是為非資料科學專業者設計的大數據領域入門課程，偏商管應用，非資訊技術教學。透過修習本課程，學員將能對資料科學商管領域的範疇與分類建立基本的觀念，並且瞭解其在商管領域的各種應用。在學的學生可藉此為職涯做準備，在職的社會人士則可拓展自己對資料科學的想像，進一步思考在自身工作場域應用資料科學的可能性。 本課程共計六週，第一週為學界與業界對談，透過直播企劃呈現大數據應用的議題，作為課程的開端，二到五週由臺灣大學教授進行授課，分別就金融、行銷、社群媒體、輿情分析、行銷智慧等議題，介紹大數據在領域的應用，課程以闡述應用為主，但不會花很多時間在演算法的技術細節。第六週則由玉山金控李正國數位金融長主講，帶入玉山金控積極應用大數據於銀行業的策略，產學合作課程確實結合學界與業界的專家，就資料科學的商管應用做不同面向的介紹。\n\n課程設計中安排一位主持人的課前提問、單元介紹引言、延伸提問等等，引導學生學習與思考，各週授課教師與課程主題概述如下：\n\n第一週：臺灣大學資訊管理學系魏志平教授、玉山金控李正國數位金融長 -- 課程簡介、與大數據的午餐約會直播活動\n第二週：臺灣大學工商管理學系與資訊管理學系合聘楊立偉教授 -- 資料分析在金融及財務上的應用\n第三週：臺灣大學工商管理學系與資訊管理學系合聘楊立偉教授 -- 資料分析在零售及行銷上的應用\n第四週：臺灣大學資訊管理學系陳建錦教授 -- 社群媒體之輿情分析\n第五週：臺灣大學資訊管理學系魏志平教授 -- 社群媒體分析與行銷智慧\n第六週：玉山金控李正國數位金融長 -- 大數據的商業應用策略",
        "translated_description_and_objectives": "本 課 是 為 的 的 的 的 的 的 的 據 據 程 偏 偏教。 本 本 本 對 對 對 對 的 的 的 的 本 本 本 本 本 本 本 本 本 本 本 本 本並 並 瞭 在 的 的 的。。。 的 的 生 生 藉 此 涯做 涯做 的 的 的 人 人士則 拓 對 對 的 的 的 的 的 的 進 一 在 在 在 在 的 的 的 的 的 的 的 的 的。。本 課 計 為 為 界 播 大 大 大 大 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的大 大學 教 課 金 金 金 金 金 銷 銷 群 群 輿 輿 輿 銷 等 介 大 大 大 大 大 的 的 的 的闡 應 應 為 不 不 花 花 間 在 的 的 的 的。。。 山 山 山 山 山 山 山 李 國 國 金 金 金 金 金 金 山 山 山 金 控 控極 大 大 據 於 的 的 的 的 的 作 作 作 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的 的\n\n安 中 中 中 一 一 位 人 的 人 的 課 紹 生 教 教 教 程 下 下\n\n第 一 ： ： 大 大學 訊 訊 理 理 志\n第 ：: 大 大學 理 與 與 與 理 聘 楊 楊 楊 楊 教 金 金 金 金 金 金 金 上 上 上 的 應\n第三 週 ： 大 大學 理 與 與 與 理 聘 楊 楊 楊 楊 教 析 析 上 上 上 上 上 上 上 上 上\n第 ： ：: 大 大學 訊 訊 理 理 錦 - 群 媒 之 之 輿 情\n第 ：: 大 大學 訊 訊 理 理 志 - 群 媒 媒 析 銷 銷\n第 ： ： ： ： 山 山 李 李 國 金 金 金融"
    },
    {
        "url": "https://www.coursera.org/learn/big-data-machine-learning",
        "original_title": "Machine Learning With Big Data",
        "translated_title": "Machine Learning With Big Data",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Want to make sense of the volumes of data you have collected?  Need to incorporate data-driven decisions into your process?  This course provides an overview of machine learning techniques to explore, analyze, and leverage data.  You will be introduced to tools and algorithms you can use to create machine learning models that learn from data, and to scale those models up to big data problems. At the end of the course, you will be able to:\n•\tDesign an approach to leverage data using the steps in the machine learning process.\n•\tApply machine learning techniques to explore and prepare data for modeling.\n•\tIdentify the type of machine learning problem in order to apply the appropriate set of techniques.\n•\tConstruct models that learn from data using widely available open source tools.\n•\tAnalyze big data problems using scalable machine learning algorithms on Spark.\n\nSoftware Requirements: \nCloudera VM, KNIME, Spark",
        "translated_description_and_objectives": "Want to make sense of the volumes of data you have collected?  Need to incorporate data-driven decisions into your process?  This course provides an overview of machine learning techniques to explore, analyze, and leverage data.  You will be introduced to tools and algorithms you can use to create machine learning models that learn from data, and to scale those models up to big data problems. At the end of the course, you will be able to:\n•\tDesign an approach to leverage data using the steps in the machine learning process.\n•\tApply machine learning techniques to explore and prepare data for modeling.\n•\tIdentify the type of machine learning problem in order to apply the appropriate set of techniques.\n•\tConstruct models that learn from data using widely available open source tools.\n•\tAnalyze big data problems using scalable machine learning algorithms on Spark.\n\nSoftware Requirements: \nCloudera VM, KNIME, Spark"
    },
    {
        "url": "https://www.coursera.org/learn/big-data-integration-processing",
        "original_title": "Big Data Integration and Processing",
        "translated_title": "Big Data Integration and Processing",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "At the end of the course, you will be able to: *Retrieve data from example database and big data management systems \n*Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications\n*Identify when a big data problem needs data integration\n*Execute simple big data integration and processing on Hadoop and Spark platforms\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+.",
        "translated_description_and_objectives": "At the end of the course, you will be able to: *Retrieve data from example database and big data management systems \n*Describe the connections between data management operations and the big data processing patterns needed to utilize them in large-scale analytical applications\n*Identify when a big data problem needs data integration\n*Execute simple big data integration and processing on Hadoop and Spark platforms\n\nThis course is for those new to data science.  Completion of Intro to Big Data is recommended.  No prior programming experience is needed, although the ability to install applications and utilize a virtual machine is necessary to complete the hands-on assignments.  Refer to the specialization technical requirements for complete hardware and software specifications.\n\nHardware Requirements: \n(A) Quad Core Processor (VT-x or AMD-V support recommended), 64-bit; (B) 8 GB RAM; (C) 20 GB disk free. How to find your hardware information: (Windows): Open System by clicking the Start button, right-clicking Computer, and then clicking Properties; (Mac): Open Overview by clicking on the Apple menu and clicking “About This Mac.” Most computers with 8 GB RAM purchased in the last 3 years will meet the minimum requirements.You will need a high speed internet connection because you will be downloading files up to 4 Gb in size. \n\nSoftware Requirements: \nThis course relies on several open-source software tools, including Apache Hadoop. All required software can be downloaded and installed free of charge (except for data charges from your internet provider). Software requirements include: Windows 7+, Mac OS X 10.10+, Ubuntu 14.04+ or CentOS 6+ VirtualBox 5+."
    },
    {
        "url": "https://www.coursera.org/professional-certificates/ibm-data-architect",
        "original_title": "IBM Data Architecture  Professional Certificate",
        "translated_title": "IBM Data Architecture  Professional Certificate",
        "original_what_youll_learn": "Build the job-ready skills you need to succeed as a data architect, including database design, data engineering, and database management., Learn how to use tools such as Airflow, Kafka, Spark, and Hadoop to build ETL workflows and process big data for analytics., Manage relational and non-relational databases, data pipelines, and data warehouses., Implement data privacy measures and governance and regulatory compliance protocols.",
        "translated_what_youll_learn": "Build the job-ready skills you need to succeed as a data architect, including database design, data engineering, and database management., Learn how to use tools such as Airflow, Kafka, Spark, and Hadoop to build ETL workflows and process big data for analytics., Manage relational and non-relational databases, data pipelines, and data warehouses., Implement data privacy measures and governance and regulatory compliance protocols.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Kickstart your career in the high-growth field of data architecture. In this program, you’ll learn in-demand skills like data modeling, database design, and enterprise data management to build scalable and secure data systems. Data architects design, implement, and manage data systems, support analytics, ensure compliance, and drive modernization. In this program, you’ll gain a strong foundation in data engineering, SQL, and relational databases (RDBMS), while building essential technical skills such as Linux commands, shell scripting, and database administration (DBA). You’ll also develop hands-on experience in key areas of the field, preparing you to work with modern data infrastructure. You’ll also explore topics like data warehousing, NoSQL databases, and ETL workflows with tools such as Airflow and Kafka. You’ll also work with big data processing using Spark and Hadoop, while covering key areas like data integration, governance, security, privacy, and compliance, ensuring you’re ready to tackle complex data challenges. When you complete this full program, you’ll have a portfolio of projects and a Professional Certificate from IBM to showcase your expertise. You’ll also earn an IBM digital badge and will gain access to career resources to help you in your job search, including mock interviews and resume support. Applied Learning Project Throughout this program, you’ll get the opportunity to apply your skills to real-world scenarios in hands-on labs and projects including: Visualize data and present findings using tools like Excel,  IBM Cognos Analytics & Tableau Create a DataFrame from a CSV file and process it using Spark SQL Design and populate a data warehouse, and model and query data using CUBE, ROLLUP, and materialized views Create ETL data pipelines with Apache Airflow and streaming data pipelines with Kafka Design a scalable Enterprise Data Architecture by creating ER diagrams for a Hotel Management System (HMS) At the end of the program, you complete a real-world capstone project specifically designed to showcase your newly learned data architect skills. This capstone project focuses on evaluating an existing data architecture and implementing data migration and integration solutions to incorporate an acquired company into the enterprise.",
        "translated_description_and_objectives": "Kickstart your career in the high-growth field of data architecture. In this program, you’ll learn in-demand skills like data modeling, database design, and enterprise data management to build scalable and secure data systems. Data architects design, implement, and manage data systems, support analytics, ensure compliance, and drive modernization. In this program, you’ll gain a strong foundation in data engineering, SQL, and relational databases (RDBMS), while building essential technical skills such as Linux commands, shell scripting, and database administration (DBA). You’ll also develop hands-on experience in key areas of the field, preparing you to work with modern data infrastructure. You’ll also explore topics like data warehousing, NoSQL databases, and ETL workflows with tools such as Airflow and Kafka. You’ll also work with big data processing using Spark and Hadoop, while covering key areas like data integration, governance, security, privacy, and compliance, ensuring you’re ready to tackle complex data challenges. When you complete this full program, you’ll have a portfolio of projects and a Professional Certificate from IBM to showcase your expertise. You’ll also earn an IBM digital badge and will gain access to career resources to help you in your job search, including mock interviews and resume support. Applied Learning Project Throughout this program, you’ll get the opportunity to apply your skills to real-world scenarios in hands-on labs and projects including: Visualize data and present findings using tools like Excel,  IBM Cognos Analytics & Tableau Create a DataFrame from a CSV file and process it using Spark SQL Design and populate a data warehouse, and model and query data using CUBE, ROLLUP, and materialized views Create ETL data pipelines with Apache Airflow and streaming data pipelines with Kafka Design a scalable Enterprise Data Architecture by creating ER diagrams for a Hotel Management System (HMS) At the end of the program, you complete a real-world capstone project specifically designed to showcase your newly learned data architect skills. This capstone project focuses on evaluating an existing data architecture and implementing data migration and integration solutions to incorporate an acquired company into the enterprise."
    },
    {
        "url": "https://www.coursera.org/learn/packt-streaming-big-data-with-spark-streaming-scala-and-spark-3-09ksx",
        "original_title": "Streaming Big Data with Spark Streaming, Scala, and Spark 3!",
        "translated_title": "Streaming Big Data with Spark Streaming, Scala, and Spark 3!",
        "original_what_youll_learn": "Identify key components of the Spark and Scala development environment., Explain the core concepts of Scala and Spark, including Resilient Distributed Datasets (RDDs) and windowing mechanisms., Differentiate between various data integration techniques with Spark Streaming, such as Kafka, Flume, and Cassandra., Assess the performance and reliability of Spark Streaming applications in production environments.",
        "translated_what_youll_learn": "Identify key components of the Spark and Scala development environment., Explain the core concepts of Scala and Spark, including Resilient Distributed Datasets (RDDs) and windowing mechanisms., Differentiate between various data integration techniques with Spark Streaming, such as Kafka, Flume, and Cassandra., Assess the performance and reliability of Spark Streaming applications in production environments.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Updated in May 2025. This course now features Coursera Coach!\n \nA smarter way to learn with interactive, real-time conversations that help you test your knowledge, challenge assumptions, and deepen your understanding as you progress through the course.\n\nIn the fast-evolving world of big data, the ability to process streaming data in real time is essential. This course is meticulously designed to take you from the basics of Spark and Scala to advanced real-time data processing with Spark Streaming. We begin with a foundational setup of your development environment, ensuring you are equipped to run Spark and Scala on your desktop. A hands-on activity will introduce you to the excitement of live data by streaming and analyzing real-time Tweets.\n\nAs we move forward, you’ll gain a solid understanding of Scala, a language integral to working with Spark. This crash course in Scala covers the essentials: variables, data structures, and flow control, with practical exercises to cement your understanding. With a firm grip on Scala, you’ll delve into the core concepts of Spark, including the Resilient Distributed Dataset (RDD), which forms the backbone of Spark Streaming applications. We will then explore Spark Streaming in detail, from its architecture to fault tolerance mechanisms, using engaging examples like tracking Twitter hashtags and analyzing Apache logs.\n\nFinally, the course pushes the boundaries of your knowledge with advanced topics such as integrating Spark Streaming with Kafka, Flume, and Cassandra. You'll also tackle stateful information tracking, real-time machine learning with K-Means clustering, and deploying your applications on a real Hadoop cluster. By the end of this course, you’ll not only understand the theory behind Spark Streaming but will have the practical experience to apply it effectively in production environments.\n\nThis course is ideal for software developers, data engineers, and data scientists with a basic understanding of programming concepts. Prior experience with Java, Python, or any object-oriented programming language is recommended but not required. Familiarity with big data concepts will be helpful but is not mandatory.",
        "translated_description_and_objectives": "Updated in May 2025. This course now features Coursera Coach!\n \nA smarter way to learn with interactive, real-time conversations that help you test your knowledge, challenge assumptions, and deepen your understanding as you progress through the course.\n\nIn the fast-evolving world of big data, the ability to process streaming data in real time is essential. This course is meticulously designed to take you from the basics of Spark and Scala to advanced real-time data processing with Spark Streaming. We begin with a foundational setup of your development environment, ensuring you are equipped to run Spark and Scala on your desktop. A hands-on activity will introduce you to the excitement of live data by streaming and analyzing real-time Tweets.\n\nAs we move forward, you’ll gain a solid understanding of Scala, a language integral to working with Spark. This crash course in Scala covers the essentials: variables, data structures, and flow control, with practical exercises to cement your understanding. With a firm grip on Scala, you’ll delve into the core concepts of Spark, including the Resilient Distributed Dataset (RDD), which forms the backbone of Spark Streaming applications. We will then explore Spark Streaming in detail, from its architecture to fault tolerance mechanisms, using engaging examples like tracking Twitter hashtags and analyzing Apache logs.\n\nFinally, the course pushes the boundaries of your knowledge with advanced topics such as integrating Spark Streaming with Kafka, Flume, and Cassandra. You'll also tackle stateful information tracking, real-time machine learning with K-Means clustering, and deploying your applications on a real Hadoop cluster. By the end of this course, you’ll not only understand the theory behind Spark Streaming but will have the practical experience to apply it effectively in production environments.\n\nThis course is ideal for software developers, data engineers, and data scientists with a basic understanding of programming concepts. Prior experience with Java, Python, or any object-oriented programming language is recommended but not required. Familiarity with big data concepts will be helpful but is not mandatory."
    },
    {
        "url": "https://www.coursera.org/specializations/applied-data-science",
        "original_title": "Applied Data Science Specialization",
        "translated_title": "Applied Data Science Specialization",
        "original_what_youll_learn": "Develop an understanding of Python fundamentals, Gain practical Python skills and apply them to data analysis, Communicate data insights effectively through data visualizations, Create a project demonstrating your understanding of applied data science techniques and tools",
        "translated_what_youll_learn": "Develop an understanding of Python fundamentals, Gain practical Python skills and apply them to data analysis, Communicate data insights effectively through data visualizations, Create a project demonstrating your understanding of applied data science techniques and tools",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This action-packed Specialization is for data science enthusiasts who want to acquire practical skills for real world data problems. If you’re interested in pursuing a career in data science, and already have foundational skills or have completed the Introduction to Data Science SpecializationOpens in a new tab, this program is for you! This 4-course Specialization will give you the tools you need to analyze data and make data driven business decisions leveraging computer science and statistical analysis. You will learn Python–no prior programming knowledge necessary–and discover methods of data analysis and data visualization. You’ll utilize tools used by real data scientists like Numpy and Pandas, practice predictive modeling and model selection, and learn how to tell a compelling story with data to drive decision making. Through guided lectures, labs, and projects in the IBM Cloud, you’ll get hands-on experience tackling interesting data problems from start to finish. Take this Specialization to solidify your Python and data science skills before diving deeper into big data, AI, and deep learning. In addition to earning a Specialization completion certificate from Coursera, you’ll also receive a digital badge from IBM. This Specialization can also be applied toward the IBM Data Science Professional CertificateOpens in a new tab. This program is ACE® recommended—when you complete, you can earn up to 12 college credits. Applied Learning Project Build your data science portfolio as you gain practical experience from producing artifacts in the interactive labs and projects throughout this program. These courses include real-world projects using principal data science tools to apply your newfound skills.\nProjects: Extract and graph financial data with the Pandas Python library. Wrangle data, graph plots, and create regression models to predict housing prices with Python libraries, including NumPy, and Sklearn. Create visualizations and a dynamic Python dashboard with treemaps and line plots using libraries such as Matplotlib, Seaborn, and Plotly Dash to monitor, report, and improve US domestic flight reliability. In the final capstone course, apply what you’ve learned from previous courses into one comprehensive project. You will train and compare machine learning models, including support vector machines, classification trees, and logistic regression, to predict if a SpaceX launch can reuse the first stage of a rocket.",
        "translated_description_and_objectives": "This action-packed Specialization is for data science enthusiasts who want to acquire practical skills for real world data problems. If you’re interested in pursuing a career in data science, and already have foundational skills or have completed the Introduction to Data Science SpecializationOpens in a new tab, this program is for you! This 4-course Specialization will give you the tools you need to analyze data and make data driven business decisions leveraging computer science and statistical analysis. You will learn Python–no prior programming knowledge necessary–and discover methods of data analysis and data visualization. You’ll utilize tools used by real data scientists like Numpy and Pandas, practice predictive modeling and model selection, and learn how to tell a compelling story with data to drive decision making. Through guided lectures, labs, and projects in the IBM Cloud, you’ll get hands-on experience tackling interesting data problems from start to finish. Take this Specialization to solidify your Python and data science skills before diving deeper into big data, AI, and deep learning. In addition to earning a Specialization completion certificate from Coursera, you’ll also receive a digital badge from IBM. This Specialization can also be applied toward the IBM Data Science Professional CertificateOpens in a new tab. This program is ACE® recommended—when you complete, you can earn up to 12 college credits. Applied Learning Project Build your data science portfolio as you gain practical experience from producing artifacts in the interactive labs and projects throughout this program. These courses include real-world projects using principal data science tools to apply your newfound skills.\nProjects: Extract and graph financial data with the Pandas Python library. Wrangle data, graph plots, and create regression models to predict housing prices with Python libraries, including NumPy, and Sklearn. Create visualizations and a dynamic Python dashboard with treemaps and line plots using libraries such as Matplotlib, Seaborn, and Plotly Dash to monitor, report, and improve US domestic flight reliability. In the final capstone course, apply what you’ve learned from previous courses into one comprehensive project. You will train and compare machine learning models, including support vector machines, classification trees, and logistic regression, to predict if a SpaceX launch can reuse the first stage of a rocket."
    },
    {
        "url": "https://www.coursera.org/learn/business-analytics-bbva",
        "original_title": "Business Analytics",
        "translated_title": "Business Analytics",
        "original_what_youll_learn": "Las fases de un proyecto basado en datos, conceptos básicos de estadística descriptiva e inferencial y estrategias para compartir la información.",
        "translated_what_youll_learn": "The phases of a data -based project, basic concepts of descriptive and inferential statistics and strategies to share information.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Hoy en día los datos son fundamentales en el entorno empresarial para tener una visión global de la situación de una organización y poder tomar mejores decisiones. En este contexto, el Big Data es un sector en auge en múltiples ámbitos que nos abre distintas oportunidades laborales, como el rol de analista de datos (Data Analyst). Este curso es una introducción al análisis de datos con un enfoque empresarial, centrado en la relevancia de basar la toma de decisiones estratégicas en el conocimiento que nos aportan los datos. \n\nA lo largo del mismo descubrirás las fases del ciclo de vida de un proyecto basado en datos, así como la importancia de asegurar la calidad de los mismos en las organizaciones. Además, aprenderás los conceptos básicos de estadística descriptiva e inferencial que todo analista de datos debe conocer. Por último, conocerás algunas estrategias para compartir información sacando el máximo rendimiento a las visualizaciones de datos. \n\nTodo ello, con el objetivo de crear impacto en las organizaciones a través de la generación de dashboards más efectivos y el empleo del storytelling a partir de los datos. \n\n¿Nos acompañas?",
        "translated_description_and_objectives": "Today the data are fundamental in the business environment to have a global vision of the situation of an organization and be able to make better decisions. In this context, Big Data is a booming sector in multiple areas that opens different job opportunities, such as the role of data analyst (Data Analyst). This course is an introduction to data analysis with a business approach, focused on the relevance of baseing the strategic decision making on the knowledge provided by the data. \n\nThroughout it you will discover the phases of the life cycle of a data -based project, as well as the importance of ensuring their quality in organizations. In addition, you will learn the basic concepts of descriptive and inferential statistics that every data analyst must know. Finally, you will know some strategies to share information by making the maximum performance to data viewing. \n\nAll this, with the aim of creating impact on organizations through the generation of more effective dashboards and the use of storytelling from the data. \n\nDo you accompany us?"
    },
    {
        "url": "https://www.coursera.org/specializations/spark-python-big-data-analysis-pyspark",
        "original_title": "Spark and Python for Big Data with PySpark Specialization",
        "translated_title": "Spark and Python for Big Data with PySpark Specialization",
        "original_what_youll_learn": "Apply PySpark to build, optimize, and evaluate distributed data processing workflows., Design and execute predictive machine learning models for large-scale analytics., Construct ETL pipelines, real-time streaming applications, and advanced big data solutions with Spark.",
        "translated_what_youll_learn": "Apply PySpark to build, optimize, and evaluate distributed data processing workflows., Design and execute predictive machine learning models for large-scale analytics., Construct ETL pipelines, real-time streaming applications, and advanced big data solutions with Spark.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This specialization provides a complete learning pathway in Apache Spark and Python (PySpark) for big data analytics, machine learning, and scalable data processing. Learners will begin with foundational Python and PySpark techniques, advance to predictive modeling and clustering, and explore advanced data workflows including ETL pipelines, streaming, and real-time processing. By the end, participants will be equipped with practical skills to design, build, and optimize distributed applications for data engineering, analytics, and business intelligence. Applied Learning Project Learners will complete hands-on projects that simulate real-world challenges such as designing ETL pipelines, building predictive ML models, segmenting customers with clustering, and processing unstructured text data. These projects ensure participants can confidently apply Spark and Python to solve authentic big data problems across industries.",
        "translated_description_and_objectives": "This specialization provides a complete learning pathway in Apache Spark and Python (PySpark) for big data analytics, machine learning, and scalable data processing. Learners will begin with foundational Python and PySpark techniques, advance to predictive modeling and clustering, and explore advanced data workflows including ETL pipelines, streaming, and real-time processing. By the end, participants will be equipped with practical skills to design, build, and optimize distributed applications for data engineering, analytics, and business intelligence. Applied Learning Project Learners will complete hands-on projects that simulate real-world challenges such as designing ETL pipelines, building predictive ML models, segmenting customers with clustering, and processing unstructured text data. These projects ensure participants can confidently apply Spark and Python to solve authentic big data problems across industries."
    },
    {
        "url": "https://www.coursera.org/specializations/microsoft-ai-enhanced-data-analysis",
        "original_title": "AI-Enhanced Data Analysis: From Raw Data to Deep Insights Specialization",
        "translated_title": "AI-Enhanced Data Analysis: From Raw Data to Deep Insights Specialization",
        "original_what_youll_learn": "Clean, manipulate, and analyze data using Excel Copilot and Python pandas for accurate insights, Create compelling visualizations and reports using advanced Excel features, Python libraries, and Power BI, Develop AI-powered data analysis workflows combining multiple Microsoft tools for comprehensive solutions",
        "translated_what_youll_learn": "Clean, manipulate, and analyze data using Excel Copilot and Python pandas for accurate insights, Create compelling visualizations and reports using advanced Excel features, Python libraries, and Power BI, Develop AI-powered data analysis workflows combining multiple Microsoft tools for comprehensive solutions",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Learn essential data analysis skills using Microsoft's leading tools including Excel with Copilot, Python, and Power BI. Clean, process, analyze, and visualize data effectively while leveraging AI-powered features for enhanced productivity. Build practical expertise in data manipulation, statistical analysis, and creating compelling visualizations that drive business insights and data-driven decision-making across multiple platforms. Applied Learning Project Complete hands-on projects using real datasets to practice data cleaning, analysis, and visualization techniques. Apply learned skills to solve authentic business problems through integrated workflows across Microsoft Excel, Python, and Power BI platforms. Description: This course delves into the world of data analysis with Python. You'll learn how to use libraries like pandas and Matplotlib to manipulate, analyze, and visualize data, extracting valuable insights and communicating findings effectively. Benefits: Become proficient in data analysis techniques, enabling you to extract meaningful insights from data and present them in compelling visualizations.\n\nBy the end of this course, you'll be able to:\n\n• Perform data cleaning, transformation, and manipulation using pandas.\n• Create various types of visualizations using Matplotlib.\n• Understand the fundamentals of generative AI and its applications in data analysis.\n• Implement basic machine learning models for data analysis.\n\nTools/Software: Python, Jupyter Notebook, pandas, Matplotlib, Scikit-learn\n\nThis course is for entry-Level professionals looking to build a foundational understanding and experience with Python, while seeking employment as a Python developer. No prior work experience or degree is required. This course provides in-depth knowledge of analytical techniques and effective data visualization using Power BI. By the end of this course, you will be able to: - Explain different analysis methods (correlation, time series, cluster, etc.) and their appropriate use.\n- Analyze data through visualizations in Power BI using different analysis methodologies\n- Leverage AI to explore data and gain insights.\n\nHere is a breakdown of what you'll cover: You will begin with statistical analysis fundamentals, master correlation, and exploratory data analysis, and understand business problems. As the course progresses, you will explore advanced topics like cluster analysis, cohort analysis, and geospatial data visualization. \n\nAdditionally, you will learn about business intelligence concepts, including building time intelligence functions, conducting time series analysis, and leveraging AI-driven tools like key influencers and decomposition trees. \n\nWith a strong emphasis on practical application, the course culminates in a hands-on final project, allowing you to synthesize your skills in a real-world scenario, making data-driven decision-making clear and impactful.\n\nThis program is for anyone interested in data analytics and visualization; there are no prerequisites. To get the most out of the learning experience, it is recommended to follow the courses in sequence, as each one builds on the skills and knowledge gained in the previous ones.  \n\nBefore starting this course, you should be proficient in building data models, maintaining relationships in Power BI, and writing DAX expressions to enhance analysis. You should also understand the ethical implications of handling data. This knowledge will be essential as you learn advanced data analysis techniques and explore data through Power BI's visualizations, supported by AI-driven insights.",
        "translated_description_and_objectives": "Learn essential data analysis skills using Microsoft's leading tools including Excel with Copilot, Python, and Power BI. Clean, process, analyze, and visualize data effectively while leveraging AI-powered features for enhanced productivity. Build practical expertise in data manipulation, statistical analysis, and creating compelling visualizations that drive business insights and data-driven decision-making across multiple platforms. Applied Learning Project Complete hands-on projects using real datasets to practice data cleaning, analysis, and visualization techniques. Apply learned skills to solve authentic business problems through integrated workflows across Microsoft Excel, Python, and Power BI platforms. Description: This course delves into the world of data analysis with Python. You'll learn how to use libraries like pandas and Matplotlib to manipulate, analyze, and visualize data, extracting valuable insights and communicating findings effectively. Benefits: Become proficient in data analysis techniques, enabling you to extract meaningful insights from data and present them in compelling visualizations.\n\nBy the end of this course, you'll be able to:\n\n• Perform data cleaning, transformation, and manipulation using pandas.\n• Create various types of visualizations using Matplotlib.\n• Understand the fundamentals of generative AI and its applications in data analysis.\n• Implement basic machine learning models for data analysis.\n\nTools/Software: Python, Jupyter Notebook, pandas, Matplotlib, Scikit-learn\n\nThis course is for entry-Level professionals looking to build a foundational understanding and experience with Python, while seeking employment as a Python developer. No prior work experience or degree is required. This course provides in-depth knowledge of analytical techniques and effective data visualization using Power BI. By the end of this course, you will be able to: - Explain different analysis methods (correlation, time series, cluster, etc.) and their appropriate use.\n- Analyze data through visualizations in Power BI using different analysis methodologies\n- Leverage AI to explore data and gain insights.\n\nHere is a breakdown of what you'll cover: You will begin with statistical analysis fundamentals, master correlation, and exploratory data analysis, and understand business problems. As the course progresses, you will explore advanced topics like cluster analysis, cohort analysis, and geospatial data visualization. \n\nAdditionally, you will learn about business intelligence concepts, including building time intelligence functions, conducting time series analysis, and leveraging AI-driven tools like key influencers and decomposition trees. \n\nWith a strong emphasis on practical application, the course culminates in a hands-on final project, allowing you to synthesize your skills in a real-world scenario, making data-driven decision-making clear and impactful.\n\nThis program is for anyone interested in data analytics and visualization; there are no prerequisites. To get the most out of the learning experience, it is recommended to follow the courses in sequence, as each one builds on the skills and knowledge gained in the previous ones.  \n\nBefore starting this course, you should be proficient in building data models, maintaining relationships in Power BI, and writing DAX expressions to enhance analysis. You should also understand the ethical implications of handling data. This knowledge will be essential as you learn advanced data analysis techniques and explore data through Power BI's visualizations, supported by AI-driven insights."
    },
    {
        "url": "https://www.coursera.org/specializations/gcp-data-machine-learning-fr",
        "original_title": "Data Engineer, Big Data and ML on Google Cloud en Français Specialization",
        "translated_title": "Data Engineer, Big Data and ML on Google Cloud en Français Specialization",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Cette formation de spécialisation en ligne d'une durée de cinq semaines présente en pratique comment concevoir et développer des systèmes de traitement des données sur Google Cloud. À travers un ensemble de présentations, de démonstrations et d'ateliers pratiques, les participants apprennent à concevoir des systèmes de traitement de données, à créer des pipelines de données de bout en bout, à analyser des données et à exécuter des tâches de machine learning. Ce cours permet aux participants d'acquérir les compétences suivantes : • Concevoir et développer des systèmes de traitement de données sur Google Cloud Platform • Exploiter des données non structurées à l'aide de Spark et des API de ML sur Cloud Dataproc • Traiter des données par lot ou par flux en mettant en œuvre des pipelines de données d'autoscaling sur Cloud Dataflow • Obtenir des informations métier à partir de très grands ensembles de données à l'aide de Google BigQuery • Entraîner, évaluer et effectuer des prédictions à l'aide de modèles de machine learning avec TensorFlow et Cloud ML • Obtenir des insights immédiats à partir de données par flux Ce cours s'adresse aux développeurs expérimentés qui se chargent de réaliser des opérations de transformation du big data. En vous inscrivant à cette spécialisation vous acceptez les conditions d'utilisation de Qwiklabs décrites dans la FAQ et disponibles à l'adresse: https://qwiklabs.com/terms_of_serviceOpens in a new tab Applied Learning Project Cette spécialisation comporte des ateliers pratiques. Pour vous y inscrire, vous devez disposer d'un compte Google (un compte Gmail suffit) et créer un compte d'essai gratuit à Google Cloud Platform. L'essai gratuit est restreint à 12 mois d'utilisation ou à 300 $ de crédit (selon la limite atteinte en premier). Nous avons donc conçu cette spécialisation pour que vous puissiez la terminer en quatre semaines. Ces ateliers vous permettent d'appliquer ce que vous apprenez dans les cours en vidéo. Les projets sont axés autour d'outils tels que Google BigQuery, qui sont utilisés et configurés dans Codelabs. Vous développerez ainsi une expérience pratique des concepts expliqués dans les modules.",
        "translated_description_and_objectives": "This online specialization training lasting five weeks presents in practice how to design and develop data processing systems on Google Cloud. Through a set of presentations, demonstrations and practical workshops, participants learn to design data processing systems, create end -to -end pipelines, analyze data and perform machine learning tasks. This course allows participants to acquire the following skills: • Design and develop data processing systems on Google Cloud Platform • operate unstructured data using Spark and ML APIs on Cloud DataProc • Process data by lot or by stream by implementing Auto Data Data Pipelines on Cloud DataFlow • Obtain business information Data using Google BigQuery • Train, evaluate and make predictions using Machine Learning models with Tensorflow and Cloud ML • Obtain immediate insights from Flux data This course is intended for experienced developers who are responsible for carrying out Big Data transformation operations. By registering for this specialization you accept the conditions of use of Qwiklabs described in the FAQ and available at the address: https://qwiklabs.com/terms_of_serviceopens in A New Tab Applied Learning Project This specialization includes practical workshops. To register, you must have a Google account (a Gmail account is enough) and create a free trial account at Google Cloud Platform. The free trial is limited to 12 months of use or $ 300 of credit (depending on the boundary first). So we designed this specialization so that you can finish it in four weeks. These workshops allow you to apply what you learn in video lessons. The projects are focused on tools such as Google Bigquery, which are used and configured in Codelabs. You will thus develop a practical experience of the concepts explained in modules."
    },
    {
        "url": "https://www.coursera.org/learn/packt-machine-9781787127081-p2-sxanh",
        "original_title": "Advanced Machine Learning, Big Data, and Deep Learning",
        "translated_title": "Advanced Machine Learning, Big Data, and Deep Learning",
        "original_what_youll_learn": "Gain expertise in dimensionality reduction and Principal Component Analysis (PCA)., Learn how to apply reinforcement learning techniques to real-world problems., Understand how to evaluate machine learning models using metrics like precision, recall, and ROC., Explore advanced deep learning models such as CNNs, RNNs, and transfer learning for various applications.",
        "translated_what_youll_learn": "Gain expertise in dimensionality reduction and Principal Component Analysis (PCA)., Learn how to apply reinforcement learning techniques to real-world problems., Understand how to evaluate machine learning models using metrics like precision, recall, and ROC., Explore advanced deep learning models such as CNNs, RNNs, and transfer learning for various applications.",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This course features Coursera Coach! A smarter way to learn with interactive, real-time conversations that help you test your knowledge, challenge assumptions, and deepen your understanding as you progress through the course.\nDive deep into advanced machine learning techniques, including data mining, dimensionality reduction, reinforcement learning, and deep learning. You'll gain hands-on experience with tools like K-Nearest Neighbors, Principal Component Analysis, and Apache Spark while working with real-world datasets. The course emphasizes key machine learning concepts such as model evaluation, cross-validation, and handling unbalanced data.\nAs you progress, you'll explore advanced neural networks like Convolutional and Recurrent Neural Networks, with practical applications such as sentiment analysis and handwriting recognition. Learn how to deploy models, use transfer learning, and understand the ethics behind machine learning and deep learning.\nThis course is ideal for anyone with a basic understanding of machine learning who wants to advance their skills with real-world applications and big data tools. Gain the expertise needed to work with cutting-edge technologies in machine learning and deep learning.\nIdeal for data scientists, machine learning engineers, and anyone with a keen interest in AI and its real-world applications.",
        "translated_description_and_objectives": "This course features Coursera Coach! A smarter way to learn with interactive, real-time conversations that help you test your knowledge, challenge assumptions, and deepen your understanding as you progress through the course.\nDive deep into advanced machine learning techniques, including data mining, dimensionality reduction, reinforcement learning, and deep learning. You'll gain hands-on experience with tools like K-Nearest Neighbors, Principal Component Analysis, and Apache Spark while working with real-world datasets. The course emphasizes key machine learning concepts such as model evaluation, cross-validation, and handling unbalanced data.\nAs you progress, you'll explore advanced neural networks like Convolutional and Recurrent Neural Networks, with practical applications such as sentiment analysis and handwriting recognition. Learn how to deploy models, use transfer learning, and understand the ethics behind machine learning and deep learning.\nThis course is ideal for anyone with a basic understanding of machine learning who wants to advance their skills with real-world applications and big data tools. Gain the expertise needed to work with cutting-edge technologies in machine learning and deep learning.\nIdeal for data scientists, machine learning engineers, and anyone with a keen interest in AI and its real-world applications."
    },
    {
        "url": "https://www.coursera.org/specializations/emerging-technologies",
        "original_title": "Emerging Technologies: From Smartphones to IoT to Big Data Specialization",
        "translated_title": "Emerging Technologies: From Smartphones to IoT to Big Data Specialization",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This Specialization is intended for researchers and business experts seeking state-of-the-art knowledge in advanced science and technology. The 4 courses cover details on Big Data (Hadoop, Spark, Storm), Smartphones, Smart Watches, Android, iOS, CPU/GPU/SoC, Mobile Communications (1G to 5G), Sensors, IoT, Wi-Fi, Bluetooth, LP-WAN, Cloud Computing, AR (Augmented Reality), Skype, YouTube, H.264/MPEG-4 AVC, MPEG-DASH, CDN, and Video Streaming Services. The Specialization includes projects on Big Data using IBM SPSS Statistics, AR applications, Cloud Computing using AWS (Amazon Web Service) EC2 (Elastic Compute Cloud), and Smartphone applications to analyze mobile communication, Wi-Fi, and Bluetooth networks. The course contents are for expert level research, design, development, industrial strategic planning, business, administration, and management. Applied Learning Project This Specialization includes various advanced projects. The Big Data project uses IBM SPSS Statistics, the AR (Augmented Reality) smartphone project analyzes limits of AR applications, the Cloud Computing project uses AWS (Amazon Web Service) EC2 (Elastic Compute Cloud), and the Smartphone project analyzes mobile communication, Wi-Fi, and Bluetooth wireless IoT networks. Every time you use Google to search something, every time you use Facebook, Twitter, Instagram or any other SNS (Social Network Service), and every time you buy from a recommended list of products on Amazon.com you are using a big data system. In addition, big data technology supports your smartphone, smartwatch, Alexa, Siri, and automobile (if it is a newer model) every day. The top companies in the world are currently using big data technology, and every company is in need of advanced big data technology support. Simply put, big data technology is not an option for your company, it is a necessity for survival and growth. So now is the right time to learn what big data is and how to use it in advantage of your company. This 6 module course first focuses on the world’s industry market share rankings of big data hardware, software, and professional services, and then covers the world’s top big data product line and service types of the major big data companies. Then the lectures focused on how big data analysis is possible based on the world’s most popular three big data technologies Hadoop, Spark, and Storm. The last part focuses on providing experience on one of the most famous and widely used big data statistical analysis systems in the world, the IBM SPSS Statistics. This course was designed to prepare you to be more successful in businesses strategic planning in the upcoming big data era. Welcome to the amazing Big Data world! Every day you use your smartphone. Your smartphone wakes you up, it is the first thing you use in the morning, and the last thing you check (e.g., alarm setting) before you sleep. In addition, you use it all day. A typical cellphone user touches their mobile phone 2,617 times a day (Dscout report based on 2017) and people spend over 4 hours a day on their mobile phones (Hackernoon report) on average. Smartphones and smart watches are very useful and will become even more useful due to their smaller sizes, lighter weights, versatile functionalities, advanced mobile communications & wireless networking (e.g., Wi-Fi & Bluetooth) technologies. In this course, the start-of-the-art smartphone and smart watch technology and components in addition to the global market trends and future forecasts are introduced. Since everybody uses smartphones and smart watches, knowing the details about the most globally used electronic device will definitely help you in all aspects of new product and app design & development, as well as business planning. In addition, the core technology and components of the world’s most popular smartphones (i.e., the Samsung Galaxy Note8 and Apple iPhone X) and smart watches (i.e., Samsung Gear S3 and the Apple Watch Series 3) are introduced along with details of the iOS and Android smartphone OSs (Operating Systems) and mobile communications 1G to 5G (for details on Wi-Fi and Bluetooth, please take my course “IoT Wireless & Cloud Emerging Technology”). This course ends with projects that teach how to analyze the components of smartphones and check the mobile network. Consequently, this course will prepare you to be more successful in businesses strategic planning in the upcoming smart device era. I cordially welcome you in to the amazing internal dynamics of the smart device world! IoT (Internet of Things) devices are already abundant, but new products that include IoT modules are now a common trend. Also, almost everything is already connected to a Cloud, and much more will be in the future. Naturally, as this trend continues, in the near future almost all devices and appliances will include IoT modules which will use sensor data collection and control/management based on Clouds. Since we will live in an IoT world supported by Clouds, knowledge of the core technologies and platforms of IoT and Clouds will enable you with the tools to become a true leader in the future product and business world. In this course, the start-of-the-art IoT and wireless networks and Cloud technologies are introduced (for details on 1G to 5G mobile communications and smartphone and smart device technology, please take my course “Smart Device & Mobile Emerging Technologies”). This course ends with projects that teach how to analyze Bluetooth and W-Fi wireless networks and setup and use an EC2 (Elastic Compute Cloud) Virtual Computer in AWS (Amazon Web Service), which is the most powerful and popular Cloud technology in the world. Comparing to the human body, IoT is the neural network and the Cloud is the brain. Thus, I cordially welcome you into the brain and neural network of the future intelligence world! Welcome to the course “Augmented Reality & Video Service Emerging Technologies.” The level of AR (Augmented Reality) and advanced video & multimedia technology included in a product is what determines the level of value and luxury. The objective of this course is to teach all important technologies that are used in state-of-the-art AR, Skype, and YouTube video and multimedia products and services. This includes the advanced video and real-time multimedia delivery mechanisms based on H.264/MPEG-4 AVC, MPEG-DASH, CDN, and mobile CDN. If you have knowledge of these core technologies, you can understand the operations that are used in every advanced video and multimedia system in the World. As the future World of business and products are driven to be more and more video and multimedia oriented, having knowledge of these core technologies will enable you to lead your company to become the true World leader in AR and video multimedia technology products, services, and business. Thus, I cordially welcome you into the beautiful and powerful World of advanced AR and video multimedia!",
        "translated_description_and_objectives": "This Specialization is intended for researchers and business experts seeking state-of-the-art knowledge in advanced science and technology. The 4 courses cover details on Big Data (Hadoop, Spark, Storm), Smartphones, Smart Watches, Android, iOS, CPU/GPU/SoC, Mobile Communications (1G to 5G), Sensors, IoT, Wi-Fi, Bluetooth, LP-WAN, Cloud Computing, AR (Augmented Reality), Skype, YouTube, H.264/MPEG-4 AVC, MPEG-DASH, CDN, and Video Streaming Services. The Specialization includes projects on Big Data using IBM SPSS Statistics, AR applications, Cloud Computing using AWS (Amazon Web Service) EC2 (Elastic Compute Cloud), and Smartphone applications to analyze mobile communication, Wi-Fi, and Bluetooth networks. The course contents are for expert level research, design, development, industrial strategic planning, business, administration, and management. Applied Learning Project This Specialization includes various advanced projects. The Big Data project uses IBM SPSS Statistics, the AR (Augmented Reality) smartphone project analyzes limits of AR applications, the Cloud Computing project uses AWS (Amazon Web Service) EC2 (Elastic Compute Cloud), and the Smartphone project analyzes mobile communication, Wi-Fi, and Bluetooth wireless IoT networks. Every time you use Google to search something, every time you use Facebook, Twitter, Instagram or any other SNS (Social Network Service), and every time you buy from a recommended list of products on Amazon.com you are using a big data system. In addition, big data technology supports your smartphone, smartwatch, Alexa, Siri, and automobile (if it is a newer model) every day. The top companies in the world are currently using big data technology, and every company is in need of advanced big data technology support. Simply put, big data technology is not an option for your company, it is a necessity for survival and growth. So now is the right time to learn what big data is and how to use it in advantage of your company. This 6 module course first focuses on the world’s industry market share rankings of big data hardware, software, and professional services, and then covers the world’s top big data product line and service types of the major big data companies. Then the lectures focused on how big data analysis is possible based on the world’s most popular three big data technologies Hadoop, Spark, and Storm. The last part focuses on providing experience on one of the most famous and widely used big data statistical analysis systems in the world, the IBM SPSS Statistics. This course was designed to prepare you to be more successful in businesses strategic planning in the upcoming big data era. Welcome to the amazing Big Data world! Every day you use your smartphone. Your smartphone wakes you up, it is the first thing you use in the morning, and the last thing you check (e.g., alarm setting) before you sleep. In addition, you use it all day. A typical cellphone user touches their mobile phone 2,617 times a day (Dscout report based on 2017) and people spend over 4 hours a day on their mobile phones (Hackernoon report) on average. Smartphones and smart watches are very useful and will become even more useful due to their smaller sizes, lighter weights, versatile functionalities, advanced mobile communications & wireless networking (e.g., Wi-Fi & Bluetooth) technologies. In this course, the start-of-the-art smartphone and smart watch technology and components in addition to the global market trends and future forecasts are introduced. Since everybody uses smartphones and smart watches, knowing the details about the most globally used electronic device will definitely help you in all aspects of new product and app design & development, as well as business planning. In addition, the core technology and components of the world’s most popular smartphones (i.e., the Samsung Galaxy Note8 and Apple iPhone X) and smart watches (i.e., Samsung Gear S3 and the Apple Watch Series 3) are introduced along with details of the iOS and Android smartphone OSs (Operating Systems) and mobile communications 1G to 5G (for details on Wi-Fi and Bluetooth, please take my course “IoT Wireless & Cloud Emerging Technology”). This course ends with projects that teach how to analyze the components of smartphones and check the mobile network. Consequently, this course will prepare you to be more successful in businesses strategic planning in the upcoming smart device era. I cordially welcome you in to the amazing internal dynamics of the smart device world! IoT (Internet of Things) devices are already abundant, but new products that include IoT modules are now a common trend. Also, almost everything is already connected to a Cloud, and much more will be in the future. Naturally, as this trend continues, in the near future almost all devices and appliances will include IoT modules which will use sensor data collection and control/management based on Clouds. Since we will live in an IoT world supported by Clouds, knowledge of the core technologies and platforms of IoT and Clouds will enable you with the tools to become a true leader in the future product and business world. In this course, the start-of-the-art IoT and wireless networks and Cloud technologies are introduced (for details on 1G to 5G mobile communications and smartphone and smart device technology, please take my course “Smart Device & Mobile Emerging Technologies”). This course ends with projects that teach how to analyze Bluetooth and W-Fi wireless networks and setup and use an EC2 (Elastic Compute Cloud) Virtual Computer in AWS (Amazon Web Service), which is the most powerful and popular Cloud technology in the world. Comparing to the human body, IoT is the neural network and the Cloud is the brain. Thus, I cordially welcome you into the brain and neural network of the future intelligence world! Welcome to the course “Augmented Reality & Video Service Emerging Technologies.” The level of AR (Augmented Reality) and advanced video & multimedia technology included in a product is what determines the level of value and luxury. The objective of this course is to teach all important technologies that are used in state-of-the-art AR, Skype, and YouTube video and multimedia products and services. This includes the advanced video and real-time multimedia delivery mechanisms based on H.264/MPEG-4 AVC, MPEG-DASH, CDN, and mobile CDN. If you have knowledge of these core technologies, you can understand the operations that are used in every advanced video and multimedia system in the World. As the future World of business and products are driven to be more and more video and multimedia oriented, having knowledge of these core technologies will enable you to lead your company to become the true World leader in AR and video multimedia technology products, services, and business. Thus, I cordially welcome you into the beautiful and powerful World of advanced AR and video multimedia!"
    },
    {
        "url": "https://www.coursera.org/specializations/gcp-data-machine-learning-br",
        "original_title": "Data Engineer, Big Data and ML on Google Cloud em Português Specialization",
        "translated_title": "Data Engineer, Big Data and ML on Google Cloud em Português Specialization",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Nesta especialização on-line intensiva de cinco semanas, os participantes terão uma introdução prática sobre como projetar e criar sistemas de processamento de dados no Google Cloud Platform. Por meio de uma combinação de apresentações, demonstrações e laboratórios práticos, os participantes aprenderão a projetar sistemas de processamento de dados, criar canais completos e análises de dados e desenvolver soluções de aprendizado de máquina. Neste curso, abordamos dados estruturados, não estruturados e de streaming. Neste curso, os participantes irão adquirir as seguintes habilidades: • projetar e criar sistemas de processamento de dados no Google Cloud Platform • usar dados não estruturados com as APIs do Spark e de aprendizado de máquina no Cloud Dataproc • processar dados em lote e streaming com a implementação de canais de dados de escalonamento automático no Cloud Dataflow • derivar insights de negócios a partir de conjuntos de dados extremamente grandes usando o Google BigQuery • treinar, avaliar e prever com modelos de aprendizado de máquina usando o TensorFlow e o Cloud ML • ativar insights instantâneos dos dados de streaming Esta aula destina-se a desenvolvedores experientes responsáveis pelo gerenciamento de transformações de Big Data. >>> Ao se inscrever nesta especialização, você concorda com os Termos de Serviço do Qwiklabs conforme estabelecido na seção de perguntas frequentes. Veja os Termos de Serviço aqui: https://qwiklabs.com/terms_of_serviceOpens in a new tab <<< Applied Learning Project Esta especialização inclui laboratórios práticos. Você precisará de uma Conta do Google (uma conta do Gmail também é aceita), além de se inscrever para uma conta de avaliação gratuita do Google Cloud Platform. A avaliação gratuita é limitada a 12 meses ou US$ 300,00 em créditos, dependendo do item que for atingido primeiro. Por esse motivo, a especialização foi desenvolvida para ser concluída em quatro semanas. Com esse treinamento prático, você poderá aplicar tudo o que aprendeu nas palestras em vídeo. Os projetos incluirão tópicos como o Google BigQuery, que são usados e configurados no Codelabs. Você ganhará experiência prática com os conceitos abordados nos módulos.",
        "translated_description_and_objectives": "In this five-week intensive online specialization, participants will have a practical introduction on how to design and create data processing systems on Google Cloud Platform. Through a combination of practical presentations, demonstrations and laboratories, participants will learn how to design data processing systems, create complete channel and data analysis, and develop machine learning solutions. In this course, we address structured, unstructured and streaming data. In this course, participants will acquire the following skills: • Design and create data processing systems on Google Cloud Platform • Use unruly data with Spark and Machine Learning APIs in Cloud Datoproc • Process batch and streaming data with the implementation of automatic scaling data on Cloud Dataflow • Derivate business insights from joints from joints. extremely large data using Google BigQuery • Train, evaluate and predict with machine learning models using Tensorflow and Cloud ML • Activate instant streaming data insights This class is intended for experienced developers responsible for Big Data transformation management. >>> By signing up for this specialization, you agree with QWIKLABS Terms of Service as set out in the Frequently asked section. See the Terms of Service here: https://qwiklabs.com/terms_of_serviceopens in a new tab <<< Applied Learning Project This specialization includes practical laboratories. You will need a Google account (a Gmail account is also accepted), and sign up for a free Google Cloud Platform assessment account. Free evaluation is limited to 12 months or US $ 300.00 in credits, depending on the item that is first reached. For this reason, the specialization was developed to be completed in four weeks. With this practical training, you can apply everything you have learned in the video lectures. Projects will include topics like Google BigQuery, which are used and configured on Codelabs. You will gain practical experience with the concepts covered in the modules."
    },
    {
        "url": "https://www.coursera.org/specializations/building-smarter-data-pipelines-sql-spark-kafka-and-genai",
        "original_title": "Building Smarter Data Pipelines: SQL, Spark, Kafka & GenAI  Specialization",
        "translated_title": "Building Smarter Data Pipelines: SQL, Spark, Kafka & GenAI  Specialization",
        "original_what_youll_learn": "Design and implement scalable data ingestion, processing, and storage systems using Apache Kafka and Spark, Build high-performance data pipelines integrating cloud platforms, databases, and generative AI technologies, Apply data engineering best practices for enterprise-scale analytics, optimization, and real-time processing",
        "translated_what_youll_learn": "Design and implement scalable data ingestion, processing, and storage systems using Apache Kafka and Spark, Build high-performance data pipelines integrating cloud platforms, databases, and generative AI technologies, Apply data engineering best practices for enterprise-scale analytics, optimization, and real-time processing",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "Master the complete data engineering pipeline from ingestion to analytics. Learn to build scalable data systems using Apache Kafka, Spark, and cloud platforms while integrating cutting-edge generative AI technologies. Apply your skills through hands-on projects that mirror real-world data engineering challenges in modern enterprises. Applied Learning Project Complete hands-on projects that simulate real enterprise data challenges. You'll build end-to-end data pipelines, implement streaming architectures with Apache Kafka and Spark, optimize database performance, design cloud-based data warehouses, and integrate generative AI for intelligent data processing, giving you practical experience with the complete modern data stack. Tired of spending hours on tedious data cleaning? Imagine if AI could handle the heavy lifting for you, turning days of work into minutes. From detecting errors to organizing vast datasets, Generative AI can not only save you time but also elevate your data quality to new heights. Dive into this course to learn how to transform data prep from a chore into a game-changing advantage! This short course was created to help you leverage Generative AI to simplify data cleaning and preparation, making workflows faster, more efficient, and accurate.\n\nBy completing this course, you’ll gain hands-on knowledge of Generative AI techniques that can be immediately applied to improve your data preparation workflows. \n\nBy the end of this 2.5-hour long course, you will be able to: \n- Identify common challenges in data cleaning and preparation that can be automated with Generative AI. \n- Apply Generative AI tools and techniques to automate repetitive data cleaning tasks, streamlining the data preparation process. \n- Evaluate the effectiveness of Generative AI in improving the efficiency and accuracy of data cleaning and preparation processes. \n- Implement specific Generative AI strategies in data cleaning workflows to minimize manual effort.\n\nThis course is unique because it combines a practical, hands-on approach with real-world case studies, enabling you to directly apply AI tools and techniques to relevant data challenges. You’ll not only explore cutting-edge AI tools but also gain valuable insights into optimizing your data preparation processes through automated solutions.\n\nTo be successful in this course, you should have a background in data handling and basic AI concepts. Experience with programming in Python and data preparation will be helpful to get the most out of the exercises.\n\nThroughout the course, you'll be assessed through a combination of practice quizzes, hands-on exercises, and a final graded assessment. These assessments are designed to ensure you've truly mastered the material and can apply your newfound knowledge to real-world scenarios. To succeed, stay engaged with the hands-on exercises, actively explore the Generative AI tools introduced, and take time to analyze the case studies provided. These activities will not only deepen your understanding but also equip you with actionable skills for immediate use in your data projects.",
        "translated_description_and_objectives": "Master the complete data engineering pipeline from ingestion to analytics. Learn to build scalable data systems using Apache Kafka, Spark, and cloud platforms while integrating cutting-edge generative AI technologies. Apply your skills through hands-on projects that mirror real-world data engineering challenges in modern enterprises. Applied Learning Project Complete hands-on projects that simulate real enterprise data challenges. You'll build end-to-end data pipelines, implement streaming architectures with Apache Kafka and Spark, optimize database performance, design cloud-based data warehouses, and integrate generative AI for intelligent data processing, giving you practical experience with the complete modern data stack. Tired of spending hours on tedious data cleaning? Imagine if AI could handle the heavy lifting for you, turning days of work into minutes. From detecting errors to organizing vast datasets, Generative AI can not only save you time but also elevate your data quality to new heights. Dive into this course to learn how to transform data prep from a chore into a game-changing advantage! This short course was created to help you leverage Generative AI to simplify data cleaning and preparation, making workflows faster, more efficient, and accurate.\n\nBy completing this course, you’ll gain hands-on knowledge of Generative AI techniques that can be immediately applied to improve your data preparation workflows. \n\nBy the end of this 2.5-hour long course, you will be able to: \n- Identify common challenges in data cleaning and preparation that can be automated with Generative AI. \n- Apply Generative AI tools and techniques to automate repetitive data cleaning tasks, streamlining the data preparation process. \n- Evaluate the effectiveness of Generative AI in improving the efficiency and accuracy of data cleaning and preparation processes. \n- Implement specific Generative AI strategies in data cleaning workflows to minimize manual effort.\n\nThis course is unique because it combines a practical, hands-on approach with real-world case studies, enabling you to directly apply AI tools and techniques to relevant data challenges. You’ll not only explore cutting-edge AI tools but also gain valuable insights into optimizing your data preparation processes through automated solutions.\n\nTo be successful in this course, you should have a background in data handling and basic AI concepts. Experience with programming in Python and data preparation will be helpful to get the most out of the exercises.\n\nThroughout the course, you'll be assessed through a combination of practice quizzes, hands-on exercises, and a final graded assessment. These assessments are designed to ensure you've truly mastered the material and can apply your newfound knowledge to real-world scenarios. To succeed, stay engaged with the hands-on exercises, actively explore the Generative AI tools introduced, and take time to analyze the case studies provided. These activities will not only deepen your understanding but also equip you with actionable skills for immediate use in your data projects."
    },
    {
        "url": "https://www.coursera.org/learn/analytics-mysql",
        "original_title": "Managing Big Data with MySQL",
        "translated_title": "Managing Big Data with MySQL",
        "original_what_youll_learn": "",
        "translated_what_youll_learn": "",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "This course is an introduction to how to use relational databases in business analysis.  You will learn how relational databases work, and how to use entity-relationship diagrams to display the structure of the data held within them.  This knowledge will help you understand how data needs to be collected in business contexts, and help you identify features you want to consider if you are involved in implementing new data collection efforts.  You will also learn how to execute the most useful query and table aggregation statements for business analysts, and practice using them with real databases. No more waiting 48 hours for someone else in the company to provide data to you – you will be able to get the data by yourself! By the end of this course, you will have a clear understanding of how relational databases work, and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of information with the hope that data will yield novel insights into how to improve businesses. Analysts that understand how to access this data – this means you! – will have a strong competitive advantage in this data-smitten business world.",
        "translated_description_and_objectives": "This course is an introduction to how to use relational databases in business analysis.  You will learn how relational databases work, and how to use entity-relationship diagrams to display the structure of the data held within them.  This knowledge will help you understand how data needs to be collected in business contexts, and help you identify features you want to consider if you are involved in implementing new data collection efforts.  You will also learn how to execute the most useful query and table aggregation statements for business analysts, and practice using them with real databases. No more waiting 48 hours for someone else in the company to provide data to you – you will be able to get the data by yourself! By the end of this course, you will have a clear understanding of how relational databases work, and have a portfolio of queries you can show potential employers. Businesses are collecting increasing amounts of information with the hope that data will yield novel insights into how to improve businesses. Analysts that understand how to access this data – this means you! – will have a strong competitive advantage in this data-smitten business world."
    },
    {
        "url": "https://www.coursera.org/specializations/serverless-data-processing-with-dataflow",
        "original_title": "Serverless Data Processing with Dataflow Specialization",
        "translated_title": "Serverless Data Processing with Dataflow Specialization",
        "original_what_youll_learn": "Demonstrate how Apache Beam and Cloud Dataflow work together to fulfill your organization’s data processing needs, Write pipelines and advanced components such as utility functions, schemas, and watermarks., Perform monitoring, troubleshooting, testing and CI/CD on Dataflow pipelines., Deploy Dataflow pipelines with reliability in mind to maximize stability for your data processing platform",
        "translated_what_youll_learn": "Demonstrate how Apache Beam and Cloud Dataflow work together to fulfill your organization’s data processing needs, Write pipelines and advanced components such as utility functions, schemas, and watermarks., Perform monitoring, troubleshooting, testing and CI/CD on Dataflow pipelines., Deploy Dataflow pipelines with reliability in mind to maximize stability for your data processing platform",
        "original_skills_youll_gain": "",
        "translated_skills_youll_gain": "",
        "original_description_and_objectives": "It is becoming harder and harder to maintain a technology stack that can keep up with the growing demands of a data-driven business. Every Big Data practitioner is familiar with the three V’s of Big Data: volume, velocity, and variety. What if there was a scale-proof technology that was designed to meet these demands? Enter Google Cloud Dataflow. Google Cloud Dataflow simplifies data processing by unifying batch & stream processing and providing a serverless experience that allows users to focus on analytics, not infrastructure. This specialization is intended for customers & partners that are looking to further their understanding of Dataflow to advance their data processing applications. This specialization contains three courses: Foundations, which explains how Apache Beam and Dataflow work together to meet your data processing needs without the risk of vendor lock-in Develop Pipelines, which covers how you convert our business logic into data processing applications that can run on Dataflow Operations, which reviews the most important lessons for operating a data application on Dataflow, including monitoring, troubleshooting, testing, and reliability. Applied Learning Project This specialization incorporates hands-on labs using Qwiklabs platform. The labs build on the concepts covered in the course modules. Where applicable, we have provided Java and Python versions of the labs. For labs that require adding/updating code, we have provided a recommended solution for your reference. This course is part 1 of a 3-course series on Serverless Data Processing with Dataflow. In this first course, we start with a refresher of what Apache Beam is and its relationship with Dataflow. Next, we talk about the Apache Beam vision and the benefits of the Beam Portability framework. The Beam Portability framework achieves the vision that a developer can use their favorite programming language with their preferred execution backend. We then show you how Dataflow allows you to separate compute and storage while saving money, and how identity, access, and management tools interact with your Dataflow pipelines. Lastly, we look at how to implement the right security model for your use case on Dataflow. Prerequisites:\nThe Serverless Data Processing with Dataflow course series builds on the concepts covered in the Data Engineering specialization. We recommend the following prerequisite courses:\n(i)Building batch data pipelines on Google Cloud : covers core Dataflow principles\n(ii)Building Resilient Streaming Analytics Systems on Google Cloud : covers streaming basics concepts like windowing, triggers, and watermarks \n\n>>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<",
        "translated_description_and_objectives": "It is becoming harder and harder to maintain a technology stack that can keep up with the growing demands of a data-driven business. Every Big Data practitioner is familiar with the three V’s of Big Data: volume, velocity, and variety. What if there was a scale-proof technology that was designed to meet these demands? Enter Google Cloud Dataflow. Google Cloud Dataflow simplifies data processing by unifying batch & stream processing and providing a serverless experience that allows users to focus on analytics, not infrastructure. This specialization is intended for customers & partners that are looking to further their understanding of Dataflow to advance their data processing applications. This specialization contains three courses: Foundations, which explains how Apache Beam and Dataflow work together to meet your data processing needs without the risk of vendor lock-in Develop Pipelines, which covers how you convert our business logic into data processing applications that can run on Dataflow Operations, which reviews the most important lessons for operating a data application on Dataflow, including monitoring, troubleshooting, testing, and reliability. Applied Learning Project This specialization incorporates hands-on labs using Qwiklabs platform. The labs build on the concepts covered in the course modules. Where applicable, we have provided Java and Python versions of the labs. For labs that require adding/updating code, we have provided a recommended solution for your reference. This course is part 1 of a 3-course series on Serverless Data Processing with Dataflow. In this first course, we start with a refresher of what Apache Beam is and its relationship with Dataflow. Next, we talk about the Apache Beam vision and the benefits of the Beam Portability framework. The Beam Portability framework achieves the vision that a developer can use their favorite programming language with their preferred execution backend. We then show you how Dataflow allows you to separate compute and storage while saving money, and how identity, access, and management tools interact with your Dataflow pipelines. Lastly, we look at how to implement the right security model for your use case on Dataflow. Prerequisites:\nThe Serverless Data Processing with Dataflow course series builds on the concepts covered in the Data Engineering specialization. We recommend the following prerequisite courses:\n(i)Building batch data pipelines on Google Cloud : covers core Dataflow principles\n(ii)Building Resilient Streaming Analytics Systems on Google Cloud : covers streaming basics concepts like windowing, triggers, and watermarks \n\n>>> By enrolling in this course you agree to the Qwiklabs Terms of Service as set out in the FAQ and located at: https://qwiklabs.com/terms_of_service <<<"
    }
]